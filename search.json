[
  {
    "objectID": "slides/index.html#what-is-gitops",
    "href": "slides/index.html#what-is-gitops",
    "title": "GitOps SDI",
    "section": "What is GitOps",
    "text": "What is GitOps\n\n“Git” & “Operations” (like DevOps)\nGit is a source of truth (apps, containers, configuration)\nA GitOps process automatically delivers changes in an online cluster.\nA monitoring process detects difference between desired state and the target cluster"
  },
  {
    "objectID": "slides/index.html#principles",
    "href": "slides/index.html#principles",
    "title": "GitOps SDI",
    "section": "5 principles",
    "text": "5 principles\n\nExpress the desired state declaratively\nVersion control the desired state\nAutomatically apply the desired state\nReconcile the desired state\nVisibility and control"
  },
  {
    "objectID": "slides/index.html#why-gitops",
    "href": "slides/index.html#why-gitops",
    "title": "GitOps SDI",
    "section": "Why GitOps?",
    "text": "Why GitOps?\nAt ISRIC - World Soil Informtion\n\nWe run various SDI’s to facilitate projects\nSDI’s should be robust, requiring little maintenance"
  },
  {
    "objectID": "slides/index.html#metadata-at-the-source",
    "href": "slides/index.html#metadata-at-the-source",
    "title": "GitOps SDI",
    "section": "Metadata at the source",
    "text": "Metadata at the source\n\nResearchers use many field and earth observation data to predict distribution of Soil properties\nThey use tools like Excel, R and Python, generally not aware of metadata standards\nAdd a README.txt to describe the resources in a folder\nWhy not structure the content in the README, so also machines can read it?"
  },
  {
    "objectID": "slides/index.html#about-the-workshop",
    "href": "slides/index.html#about-the-workshop",
    "title": "GitOps SDI",
    "section": "About the workshop",
    "text": "About the workshop\n\nA self guided tutorial at https://pvgenuchten.github.io/training-gitops-sdi/\nTwo moments to summarize\nWelcoming your feedback at https://forms.gle/J59DjGbTBx1s1Tus7\nAsk questions via https://github.com/pvgenuchten/training-gitops-sdi/discussions/5"
  },
  {
    "objectID": "slides/index.html#technologies",
    "href": "slides/index.html#technologies",
    "title": "GitOps SDI",
    "section": "Technologies",
    "text": "Technologies\n\nMCF\npygeometa\npyGeoDataCrawler\npycsw\nmapserver\nterriajs"
  },
  {
    "objectID": "slides/index.html#metadata-control-file",
    "href": "slides/index.html#metadata-control-file",
    "title": "GitOps SDI",
    "section": "Metadata Control File",
    "text": "Metadata Control File\n\nA convention of the pygeometa community (Meteo Canada)\nYAML, easy to read by humans and machines\nA subset of the ISO19115 standard for metadata\n\n\nMCF"
  },
  {
    "objectID": "slides/index.html#pygeometa",
    "href": "slides/index.html#pygeometa",
    "title": "GitOps SDI",
    "section": "pygeometa",
    "text": "pygeometa\n\na library to work with MCF\nexports MCF to various metadata models (iso, dcat, stac)\nhttps://github.com/geopython/pygeometa"
  },
  {
    "objectID": "slides/index.html#pygeodatacrawler",
    "href": "slides/index.html#pygeodatacrawler",
    "title": "GitOps SDI",
    "section": "pyGeoDataCrawler",
    "text": "pyGeoDataCrawler\n\nDeveloped at ISRIC in the last year\nOn the shoulders of giants; GDAL, pygeometa, pandas\nGenerates MCF files for a file repository\nImports metadata from external sources\nExtracts MCF files from a repository to load it into a catalogue\nCreates map services configuration for files in the repository"
  },
  {
    "objectID": "slides/index.html#pycsw-catalogue",
    "href": "slides/index.html#pycsw-catalogue",
    "title": "GitOps SDI",
    "section": "pycsw catalogue",
    "text": "pycsw catalogue\n\nA catalogue implementation in Python\nLimited funtionality, easy maintenance\nWide range of supported catalogue standards, serves many communities\nDatabase backend\nSkin customisable with Jinja templates\n\nRelated products; GeoNetwork, CKAN, dataverse"
  },
  {
    "objectID": "slides/index.html#mapserver",
    "href": "slides/index.html#mapserver",
    "title": "GitOps SDI",
    "section": "MapServer",
    "text": "MapServer\n\nRecent versions of MapServer support the old and new standards\nFeatures and Maps\nNo tiles (mapproxy), No coverages yet\nNo records/stac, by design\n\nRelated products; GeoServer, QGIS server, pygeopai"
  },
  {
    "objectID": "slides/index.html#terriajs",
    "href": "slides/index.html#terriajs",
    "title": "GitOps SDI",
    "section": "TerriaJS",
    "text": "TerriaJS\nAn Open Source javascript WebGIS application\nRelated products; GeoNode, GeoMoose, MapBender, Oskari, GisQuick"
  },
  {
    "objectID": "slides/index.html#quarto",
    "href": "slides/index.html#quarto",
    "title": "GitOps SDI",
    "section": "Quarto",
    "text": "Quarto\n\nStatic site generator from Markdown\nAdd diagrams or maps with Python and R scripting\nOther interesting option is the W3C MapML"
  },
  {
    "objectID": "8-data-visualization.html",
    "href": "8-data-visualization.html",
    "title": "Advanced options",
    "section": "",
    "text": "TerriaJS is a modern Web GIS application, which includes an option to query a CSW catalogue.\nThe terriamap Docker image is available via the GitHub Container Registry (ghcr.io)\ndocker run -p 3001:3001 ghcr.io/terriajs/terriamap:0.4.2\nVisit http://localhost:3001 to see TerriaJS in action.",
    "crumbs": [
      "Home",
      "Data visualization"
    ]
  },
  {
    "objectID": "8-data-visualization.html#terriajs",
    "href": "8-data-visualization.html#terriajs",
    "title": "Advanced options",
    "section": "",
    "text": "TerriaJS is a modern Web GIS application, which includes an option to query a CSW catalogue.\nThe terriamap Docker image is available via the GitHub Container Registry (ghcr.io)\ndocker run -p 3001:3001 ghcr.io/terriajs/terriamap:0.4.2\nVisit http://localhost:3001 to see TerriaJS in action.",
    "crumbs": [
      "Home",
      "Data visualization"
    ]
  },
  {
    "objectID": "8-data-visualization.html#terriajs-within-docker-compose",
    "href": "8-data-visualization.html#terriajs-within-docker-compose",
    "title": "Advanced options",
    "section": "TerriaJS within Docker Compose",
    "text": "TerriaJS within Docker Compose\nIn our Docker Compose setup, terriamap is routed via localhost/map.\nTwo files are mounted into the container:\n\nconfig.json contains the configuration of the interface, you should add a personalised Google/Bing/Cesium key here.\nsimple.json is the configuration of the inital map (you can add more map configurations in a smilar way). A reference to MapServer and pycsw are included.\nIn index.html the title/abstract of the page can be customized. Also, because TerriaJS is hosted at /map, the base url is set to /map.\n\nNotice that from terria layer details, you can access the record in the catalogue (link is shared via WMS Capabilities)",
    "crumbs": [
      "Home",
      "Data visualization"
    ]
  },
  {
    "objectID": "8-data-visualization.html#summary",
    "href": "8-data-visualization.html#summary",
    "title": "Advanced options",
    "section": "Summary",
    "text": "Summary\nWith this topic we conclude our training on data management. We hope you enjoyed the materials. Notice that the training can act as a starting point to a number of other resources. Let us know via GitHub issues if you have improvement suggestions for the materials.",
    "crumbs": [
      "Home",
      "Data visualization"
    ]
  },
  {
    "objectID": "6-data-publication.html",
    "href": "6-data-publication.html",
    "title": "Data publication",
    "section": "",
    "text": "In order to share a dataset with colleagues, partners or the wider public. The file should be published in a shared environment. Various technologies are available to share a file on a network. To select a relevant location mainly depends on which type of users are going to access the dataset.\nThe following options exist:\n\nA data repository such as Zenodo or Dataverse. With this option metadata of the resource is automatically collected and searchable.\nA cloud service such as Google Drive, Microsoft Sharepoint, Dropbox, Amazon Webservices, GitHub. Such a service can also be setup locally. A minimal solution would be to set up a Webdav service.\nA shared folder on a central server on the local intranet. Notice that this location is usually not available by remote partners.",
    "crumbs": [
      "Home",
      "Data publication"
    ]
  },
  {
    "objectID": "6-data-publication.html#introduction",
    "href": "6-data-publication.html#introduction",
    "title": "Data publication",
    "section": "",
    "text": "In order to share a dataset with colleagues, partners or the wider public. The file should be published in a shared environment. Various technologies are available to share a file on a network. To select a relevant location mainly depends on which type of users are going to access the dataset.\nThe following options exist:\n\nA data repository such as Zenodo or Dataverse. With this option metadata of the resource is automatically collected and searchable.\nA cloud service such as Google Drive, Microsoft Sharepoint, Dropbox, Amazon Webservices, GitHub. Such a service can also be setup locally. A minimal solution would be to set up a Webdav service.\nA shared folder on a central server on the local intranet. Notice that this location is usually not available by remote partners.",
    "crumbs": [
      "Home",
      "Data publication"
    ]
  },
  {
    "objectID": "6-data-publication.html#persistent-identification",
    "href": "6-data-publication.html#persistent-identification",
    "title": "Data publication",
    "section": "Persistent identification",
    "text": "Persistent identification\nIt is important that datasets made available for reuse remain available at a persistent location. So any documentation, which reference the dataset as a source, doesn’t break. Repositories typically provide a persistent identification layer between the deposited dataset and the users (such as DOI or ePIC). In case a file is moved, providers can update the DOI reference to the new location.",
    "crumbs": [
      "Home",
      "Data publication"
    ]
  },
  {
    "objectID": "6-data-publication.html#include-metadata",
    "href": "6-data-publication.html#include-metadata",
    "title": "Data publication",
    "section": "Include metadata",
    "text": "Include metadata\nFor optimal discoverability, it is important to combine data publication with metadata. Either via embedded metadata in the file, else with a separate metadata file. In case of a shared folder or cloud service, embed or place the metadata side by side with the data files, so people browsing through the system can easily find it.\nThe embedded or sidecar metadata can be ingested by catalogue software, to make it searchable for the targeted audience. This process is further described at catalogue publication.",
    "crumbs": [
      "Home",
      "Data publication"
    ]
  },
  {
    "objectID": "6-data-publication.html#summary",
    "href": "6-data-publication.html#summary",
    "title": "Data publication",
    "section": "Summary",
    "text": "Summary\nVarious technologies exist to share data on a network. When selecting a mechanism, evaluate if you can facilitate identifier persistence and share metadata along with the files. In the next section we’ll setup convenience APIs on data to facilitate reuse of the data.",
    "crumbs": [
      "Home",
      "Data publication"
    ]
  },
  {
    "objectID": "4-bulk-import.html",
    "href": "4-bulk-import.html",
    "title": "Bulk import",
    "section": "",
    "text": "This paragraph describes approaches to import metadata from existing repositories. Including an option to import metadata from records of a spreadsheet.",
    "crumbs": [
      "Home",
      "Bulk import"
    ]
  },
  {
    "objectID": "4-bulk-import.html#bulk-import-from-a-spreadsheet",
    "href": "4-bulk-import.html#bulk-import-from-a-spreadsheet",
    "title": "Bulk import",
    "section": "Bulk import from a spreadsheet",
    "text": "Bulk import from a spreadsheet\nMany metadata initiatives tend to start from a spreadsheet. Each of the columns representa a metadata property and the rows are the individual records describing a resource. Spreadsheets have proven to be an effective medium to populate a catalogue with records initially. To facilitate this use case the pyGeoDataCrawler software provides an import spreadsheet method. The spreadsheet is parsed and a MCF document is generated for every row.\nSince every metadata initiative tends to have dedicated columns. A templating approach is used to convert from row to MCF. A default template is available, matching a default spreadsheet layout. If your spreadsheet layout is different, you need to adjust the template accordingly.\n\nFor this exercise we’ll use index.csv and index.j2 file in .data/csv. Notice that the template has the same filename, but with extension .j2. Navigate to the folder called csv, in your working directory.\nFrom your shell environment run this command:\n\n\nLocalDocker & LinuxDocker & PowerShell\n\n\ncrawl-metadata --mode=import-csv --dir=\"./csv\"\n\n\ndocker run -it --rm -v $(pwd):/tmp \\\n  pvgenuchten/geodatacrawler crawl-metadata \\\n  --mode=import-csv --dir=\"/tmp\" --sep=\";\"\n\n\ndocker run -it --rm -v ${PWD}:/tmp `\n  pvgenuchten/geodatacrawler crawl-metadata `\n  --mode=import-csv --dir=\"/tmp\" --sep=\";\"\n\n\n\n\nIf there are errors, check the paths and consider to open the CSV in Google Sheets and export it again or open it in a text editor to look for special cases. A known issue with this approach is that the crawler tool can not manage newline characters in text fields.\nOpen one of the generated MCF files to evaluate its content.\nA common spreadsheet tool is Microsoft Excel. If you open and export a spreadsheet from Excel, the CSV will use the ‘;’ character as column separator. Use the –sep=‘;’ parameter to indicate pyGeoDataCrawler to use this separator.",
    "crumbs": [
      "Home",
      "Bulk import"
    ]
  },
  {
    "objectID": "4-bulk-import.html#bulk-import-from-an-online-location",
    "href": "4-bulk-import.html#bulk-import-from-an-online-location",
    "title": "Bulk import",
    "section": "Bulk import from an online location",
    "text": "Bulk import from an online location\nMany resources are already described elsewhere which may be of interest to add to our catalogue. For this use case some options exist to import remote metadata.\nIn case you want to harvest the full set of a remote catalogue, you can create a basic MCF in a new folder undrr and add a distribution of a CSW endpoint.\nmetadata:\n    hierarchylevel: service\n    identifier: riskprofilesundrr\ndistribution:\n  csw:\n    name: csw\n    url: http://riskprofilesundrr.org/catalogue/csw\n    type: OGC:CSW\nNow use the crawler to fetch the remote records of the catalogue.\ncrawl-metadata --mode=update --dir=\"./undrr\" --resolve=true\nYou can repeat this harvest at intervals to keep your catalogue up to date with the remote.",
    "crumbs": [
      "Home",
      "Bulk import"
    ]
  },
  {
    "objectID": "4-bulk-import.html#summary",
    "href": "4-bulk-import.html#summary",
    "title": "Bulk import",
    "section": "Summary",
    "text": "Summary\nWe’ve seen a number of options to import metadata from external sources. In the next section we’ll have a look at Git, a versioning system.",
    "crumbs": [
      "Home",
      "Bulk import"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html",
    "href": "2-interact-with-data-repositories.html",
    "title": "Interact with data repositories",
    "section": "",
    "text": "In this section a crawler tool is introduced which let’s you interact with the metadata in a file based data repository. For this exercise we’ve prepared a minimal data repository containing a number of Excel-, Shape- and Tiff-files. Unzip the repository to a location on disk.\nIn the root folder of the repository already exists a minimal MCF file, index.yml. This file contains some generic metadata properties which are used if a file within the repository does not include them. The tool we use is able to inherit metadata properties from this index.yml file through the file hierarchy.\n\n\n\n\n\n\nTip\n\n\n\nOpen index.yml and customise the contact details. Later you will notice that these details will be applied to all datasets which themselves do not provide contact details.\n\n\nConsider to add additional index.yml files in other folders to override the values of index.yml at top level.",
    "crumbs": [
      "Home",
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#introduction",
    "href": "2-interact-with-data-repositories.html#introduction",
    "title": "Interact with data repositories",
    "section": "",
    "text": "In this section a crawler tool is introduced which let’s you interact with the metadata in a file based data repository. For this exercise we’ve prepared a minimal data repository containing a number of Excel-, Shape- and Tiff-files. Unzip the repository to a location on disk.\nIn the root folder of the repository already exists a minimal MCF file, index.yml. This file contains some generic metadata properties which are used if a file within the repository does not include them. The tool we use is able to inherit metadata properties from this index.yml file through the file hierarchy.\n\n\n\n\n\n\nTip\n\n\n\nOpen index.yml and customise the contact details. Later you will notice that these details will be applied to all datasets which themselves do not provide contact details.\n\n\nConsider to add additional index.yml files in other folders to override the values of index.yml at top level.",
    "crumbs": [
      "Home",
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#setup-environment",
    "href": "2-interact-with-data-repositories.html#setup-environment",
    "title": "Interact with data repositories",
    "section": "Setup environment",
    "text": "Setup environment\nThe tools we will use are based on Python. For this workshop we offer 2 approaches:\n\nIf you are familiar with Python or are interested to learn more about it, you can setup a Conda or Python virtual environment and run the scripts in that environment.\nElse, we recommend to run the Python tools in Docker containers.\n\nEach of the exercises indicates an option to run the Python script directly, or from a container.\n\nConda\nThe tools have some specific dependencies which are best installed via Conda. Conda creates for each project a virtual environment, so any activity will not interfere with the base environment of your machine. Conda manages both the Python libraries as well as any other dependencies, such as GDAL.\nIf you don’t use Conda yet, consider to install mamba, a lighter alternative to Conda. Or alternatively run the scripts in a Python virtual environment.\nStart a commandline or PowerShell with mamba enabled (or add mamba to your PATH). On windows look for the Mamba prompt in start menu. First we will navigate to the folder in which we unzipped the sample data repository. Make sure you are not in the data directory but one above.\ncd {path-where-you-unzipped-zipfile}\nWe will create a virtual environment pgdc (using Python 3.10) for our project and activate it.\nmanba create --name pgdc python=3.10 \nmamba activate pgdc\nNotice that you can deactivate this environment with: mamba deactivate and you will return to the main Python environment. The tools we will install below, will not be available in the main environment.\nInstall the dependencies for the tool:\nmamba install -c conda-forge gdal\nmamba install -c conda-forge pysqlite3\nInstall the crawler tool, GeoDataCrawler. The tool is under active development at ISRIC and facilitates many of our data workflows. It is powered by some popular metadata and transformation libraries; OWSLib, pygeometa and GDAL. If you want to learn more about these libraries, consider to read the ‘tutorials’ of these libraries (OWSlib, pygeometa, gdal), or enjoy the jupyter based geopyton workshop.\npip3 install pyGeoDataCrawler\nVerify the different crawling options by typing:\ncrawl-metadata --help\n\n\nPython/GDAL via Docker\nIn case you have difficulties setting up Python with GDAL on your local machine (or just want to try out), an alternative approach is available using Python via Docker. Docker is a virtualization technology which runs isolated containers within your computer.\n\nFirst install docker.\nStart the Docker Desktop tool.\nNow navigate to the folder where you unzipped the data repository and use the Docker image to run the crawler:\n\ndocker run -it --rm pvgenuchten/geodatacrawler crawl-metadata --help\nFor advanced Docker statements there are some differences between Windows commandline, Windows PowerShell and Linux bash. Use the relevant syntax for your system.",
    "crumbs": [
      "Home",
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#initial-mcf-files",
    "href": "2-interact-with-data-repositories.html#initial-mcf-files",
    "title": "Interact with data repositories",
    "section": "Initial MCF files",
    "text": "Initial MCF files\nThe initial task for the tool is to create for every data file in our repository a sidecar file based on embedded metadata from the resource.\n\nLocalDocker & LinuxDocker & PowerShell\n\n\ncrawl-metadata --mode=init --dir=data\n\n\ndocker run -it --rm -v $(pwd):/tmp \\\n pvgenuchten/geodatacrawler crawl-metadata \\\n --mode=init --dir=/tmp/data\n\n\ndocker run -it --rm -v ${PWD}:/tmp `\n  pvgenuchten/geodatacrawler crawl-metadata `\n  --mode=init --dir=/tmp/data\n\n\n\nNotice that for each resource a {dataset}.yml file has been created. Open a .yml file in a text editor and review the content. Notice on the Docker statements that we mount the local folder into the container, before we can run commands into it. To verify if the correct folder was mounted, run a ls command to see the folder contents.\n\nDocker & LinuxDocker & PowerShell\n\n\ndocker run -it --rm -v $(pwd):/tmp pvgenuchten/geodatacrawler ls /tmp\n\n\ndocker run -it --rm -v ${PWD}:/tmp pvgenuchten/geodatacrawler ls /tmp",
    "crumbs": [
      "Home",
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#update-mcf",
    "href": "2-interact-with-data-repositories.html#update-mcf",
    "title": "Interact with data repositories",
    "section": "Update MCF",
    "text": "Update MCF\nThe update mode is meant to be run at intervals, it will update the MCF files if changes have been made on a resource.\n\nLocalDocker & LinuxDocker & PowerShell\n\n\ncrawl-metadata --mode=update --dir=data\n\n\ndocker run -it --rm -v $(pwd):/tmp \\\n pvgenuchten/geodatacrawler crawl-metadata \\\n --mode=update --dir=/tmp/data\n\n\ndocker run -it --rm -v ${PWD}:/tmp `\n  pvgenuchten/geodatacrawler crawl-metadata `\n  --mode=update --dir=/tmp/data\n\n\n\nIn certain cases the update mode will also import metadata from remote url’s. This happens for example if the dataset-uri is a DOI. The update mode will ten fetch metadata of the DOI and push it into the MCF.",
    "crumbs": [
      "Home",
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#export-mcf",
    "href": "2-interact-with-data-repositories.html#export-mcf",
    "title": "Interact with data repositories",
    "section": "Export MCF",
    "text": "Export MCF\nFinally we want to export the MCF’s to actual iso19139 metadata to be loaded into a catalogue like pycsw, GeoNetwork, CKAN etc.\n\nLocalDocker & LinuxDocker & PowerShell\n\n\ncrawl-metadata --mode=export --dir=data --dir-out=export --dir-out-mode=flat\n\n\ndocker run -it --rm -v $(pwd):/tmp \\\n pvgenuchten/geodatacrawler crawl-metadata \\\n --mode=export --dir=/tmp/data \\\n --dir-out=/tmp/export --dir-out-mode=flat\n\n\ndocker run -it --rm -v ${PWD}:/tmp `\n  pvgenuchten/geodatacrawler crawl-metadata `\n  --mode=export --dir=/tmp/data `\n  --dir-out=/tmp/export --dir-out-mode=flat\n\n\n\nOpen one of the xml files and evaluate if the contact information from step 1 is available.",
    "crumbs": [
      "Home",
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#summary",
    "href": "2-interact-with-data-repositories.html#summary",
    "title": "Interact with data repositories",
    "section": "Summary",
    "text": "Summary\nIn this paragraph you have been introduced to the pyGeoDataCrawler library. In the next section we are looking at catalogue publication.",
    "crumbs": [
      "Home",
      "Data repositories"
    ]
  },
  {
    "objectID": "1-metadata-at-the-source.html",
    "href": "1-metadata-at-the-source.html",
    "title": "Metadata at the source",
    "section": "",
    "text": "Many organizations organise their documents and datasets at a central network location or database. These resources are usually clustered in organizational units, projects and/or years. Some files and database tables in that central network location contain embedded metadata, such as the name, size, date, author, location etc. This information supports users in understanding the context of the data source. Especially if that data at some point is migrated from its original context.",
    "crumbs": [
      "Home",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "1-metadata-at-the-source.html#introduction",
    "href": "1-metadata-at-the-source.html#introduction",
    "title": "Metadata at the source",
    "section": "",
    "text": "Many organizations organise their documents and datasets at a central network location or database. These resources are usually clustered in organizational units, projects and/or years. Some files and database tables in that central network location contain embedded metadata, such as the name, size, date, author, location etc. This information supports users in understanding the context of the data source. Especially if that data at some point is migrated from its original context.",
    "crumbs": [
      "Home",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "1-metadata-at-the-source.html#sidecar-metadata",
    "href": "1-metadata-at-the-source.html#sidecar-metadata",
    "title": "Metadata at the source",
    "section": "Sidecar metadata",
    "text": "Sidecar metadata\nFor those formats which lack embedded metadata, or in order to capture additional metadata aspects, you may consider creating a sidecar metadata file for every resource. A dedicated metadata file sharing the name of the datasource. This approach is for example common in the Esri community, where a .shp.xml is created alongside any .shp file, which captures relevant metadata elements.\n\n\n\n\n\n\nTip\n\n\n\nLocate on your local computer or network drive a random shapefile. Does the file have a .shp.xml sidecar file? Else find another shape or tiff file (look for *.shp.xml). The contents of the xml file may be very minimal, but in most cases at least some processing information and the data model of the shapefile are mentioned.\n\n\nThrough the embedded metadata and sidecar concept, we endorse data scientists to document their data at the source. Since the data producers are often best informed how the data is produced and how it should be used.",
    "crumbs": [
      "Home",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "1-metadata-at-the-source.html#standards-and-interoperability",
    "href": "1-metadata-at-the-source.html#standards-and-interoperability",
    "title": "Metadata at the source",
    "section": "Standards and interoperability",
    "text": "Standards and interoperability\nFor optimal interoperability, it is important to agree within your group on the metadata standard(s) to use in sidecar files. Esri software for example provides an option to select the model of the metadata as documented in the ArcGIS Pro documentation. QGIS has various plugins, such as GeoCat Bridge, to work with various metadata models.\n\n\n\n\n\n\nTip\n\n\n\nDoes your organization or community endorse a metadata model to describe data sources? Are you aware of tooling which can support you in creation of metadata in this model?",
    "crumbs": [
      "Home",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "1-metadata-at-the-source.html#getting-started",
    "href": "1-metadata-at-the-source.html#getting-started",
    "title": "Metadata at the source",
    "section": "Getting started",
    "text": "Getting started\nWithin the geopython community, the pygeometa library provides a metadata format called the metadata control file (MCF). The aim of MCF is ease of use, while providing export options to various metadata models. Many metadata models are based on XML, which makes them quite challenging to read by humans. MCF is based on YAML, a text-based format using indents to group elements. In this workshop we are using the MCF format for its simplicity and natural fit with the use cases. A minimal sample of an MCF is:\nmcf:\n    version: 1.0\n\nmetadata:\n    identifier: 9c36a048-4d28-453f-9373-94c90e101ebe\n    hierarchylevel: dataset\n    date: 2023-05-10\n\nidentification:\n    title: My favourite dataset\n    abstract: A sample dataset record to highlight the options of MCF\n    ...\nIf you are comfortable with Python, consider to try the following experiment.\n\n\n\n\n\n\nTip\n\n\n\nDownload an MCF sample pygeometa repository. Then open a shell and set up a virtual Python (or Conda) environment, then:\npip3 install pygeometa\npygeometa metadata info path/to/sample.yml\npygeometa metadata generate path/to/sample.yml --schema=iso19139 --output=md.xml\n\n\nRead more about pygeometa at https://geopython.github.io/pygeometa/tutorial/.\nWhen describing a resource, consider which user groups are expected to read the information. This analyses will likely impact the style of writing in the metadata. The UK Geospatial Commission has published some practical recommendations on this topic.\nWhen tagging the dataset with keywords, preferably use keywords from controlled vocabularies like Agrovoc, Eurovoc, etc. A benefit of controlled vocabularies is that the term is not ambigue and it can be processed in multiple languages.",
    "crumbs": [
      "Home",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "1-metadata-at-the-source.html#mcf-editing",
    "href": "1-metadata-at-the-source.html#mcf-editing",
    "title": "Metadata at the source",
    "section": "MCF editing",
    "text": "MCF editing\nMCF documents can be written in a text editor like Visual Studio Code. Consider to install the YAML plugin for instant YAML validation.\nAnother option to create and update MCF files is via MDME. MDME is a web based software package providing a dynamic metadata edit form. An operational package is available at osgeo.github.io. Notice that if you install the package locally, you can customize the metadata model to your organizational needs.\n\n\n\n\n\n\nTip\n\n\n\nImagine a dataset you have recently worked with. Then open mdme and populate the form, describing that dataset. Now save the MCF file so we can later place it in a sample data repository.\nNotice that MDME also offers capabilities to export directly as iso19139, it uses a webservice based on the tools used in this workshop.",
    "crumbs": [
      "Home",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "1-metadata-at-the-source.html#summary",
    "href": "1-metadata-at-the-source.html#summary",
    "title": "Metadata at the source",
    "section": "Summary",
    "text": "Summary\nIn this section, you are introduced to a data management approach which maintains metadata at the location where the datasets are maintained, using a minimal, standards complient approach. You are introduced to the MCF metadata format. In the next section, we will go into more detail on interacting with the MCF format.",
    "crumbs": [
      "Home",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "3-catalogue-publication.html",
    "href": "3-catalogue-publication.html",
    "title": "Catalogue publication",
    "section": "",
    "text": "Catalogues facilitate data discovery in 3 ways:",
    "crumbs": [
      "Home",
      "Catalogue publication"
    ]
  },
  {
    "objectID": "3-catalogue-publication.html#pycsw-catalogue",
    "href": "3-catalogue-publication.html#pycsw-catalogue",
    "title": "Catalogue publication",
    "section": "pycsw catalogue",
    "text": "pycsw catalogue\nVarious catalogue frontends exist to facilitate dataset search, such as Geonetwork OpenSource, dataverse, CKAN. Selecting a frontend depends on metadata format, target audience, types of data, maintenance aspects, and personal preference.\nFor this workshop we are going to use pycsw. It is a catalogue software supporting various standardised query APIs, as well as providing a basic easy-to-adjust html web interface.\nFor this exercise we assume you have Docker Desktop installed on your system and running. Visit the Docker get started tutorials in case you’re new to docker.\npycsw is available as Docker image at the github container registry, including an embedded SQLite database. In a production situation you will instead use a dedicated Postgres or MariaDB database for record storage. Notice that when you destroy the container, the SQLite database will be set to its default content.\nPull and run the pycsw container locally using this command in a command line client (cmd, PowerShell, bash):\ndocker run -p 8000:8000 geopython/pycsw@sha256:2eb396798e40dfab3ad4cb839f30231b8c1023db148ed03ffcf3c4e6b388fc7c\nOpen your browser and browse to http://localhost:8000 to see pycsw in action.\nReturn to the command line, press ctrl-C to stop the Docker container process.",
    "crumbs": [
      "Home",
      "Catalogue publication"
    ]
  },
  {
    "objectID": "3-catalogue-publication.html#docker-compose",
    "href": "3-catalogue-publication.html#docker-compose",
    "title": "Catalogue publication",
    "section": "Docker Compose",
    "text": "Docker Compose\nCompose is a utility of docker, enabling setup of a set of containers using a composition script. A composition script can automate the manual startup operations of the previous paragraph. We’ve prepared a composition script for this workshop. The script includes, besides the pycsw container, other containers from next paragraphs.\nClone the workshop repository to a local folder (You don’t have Git installed? You can also download the repository as a zip file).\ngit clone https://github.com/pvgenuchten/training-gitops-sdi.git\nOn the cloned repository in the docker folder there are 2 alternatives:\n\ndocker-compose.yml is the full orchestration including PostGIS and TerriaJS\ndocker-compose.sqlite.yml is a minimal orchestration without terria and based on a file based SQLite database\n\nOn both orchestrations a library is used called Traefik to facilitate path-routing to the relavant containers.\nAlso notice that some layout templates are mounted into the pycsw container. These templates override the default layout of pycsw.\nSome environment variables should be set in a .env file. Rename the .env-template file to .env.\nThen open a shell and navigate to the Docker folder in the cloned repository and run:\ndocker compose -f docker-compose.sqlite.yml up\nA lot of logs are produced by the various containers. You can also run in the background (-d or --detach) using:\ndocker compose -f docker-compose.sqlite.yml up -d\nWhen running in the background, use docker compose down, docker ps, docker logs pycsw to stop, see active containers and see the logs of a container. Or interact with the containers from Docker Desktop.",
    "crumbs": [
      "Home",
      "Catalogue publication"
    ]
  },
  {
    "objectID": "3-catalogue-publication.html#load-some-records",
    "href": "3-catalogue-publication.html#load-some-records",
    "title": "Catalogue publication",
    "section": "Load some records",
    "text": "Load some records\nMake sure the Docker setup is running in the background (-d), or open a second shell window.\nMuch of the configuration of pycsw (title, contact details, database connection, url) is managed in a config file. You will find a copy of this file in ./docker/pycsw. In this file, adjust the catalogue title and restart the orchestration. Notice the updated title in your browser.\nFor administering the contents of the catalogue a utility called pycsw-admin.py is available in the pycsw container. You can either open a shell in the container (via Docker Desktop) and type the commands directly, or use docker exec to run the commands from the host.\nFirst clear the existing database:\n\nContainer terminalPowerShell\n\n\npycsw-admin.py delete-records -c /etc/pycsw/pycsw.yml\n\n\ndocker exec -it pycsw bash -c \"pycsw-admin.py delete-records -c /etc/pycsw/pycsw.yml\"\n\n\n\nNotice at http://localhost:8000/collections/metadata:main/items that all records are removed.\nWe exported MCF records as iso19139 in the previous section. Copy the ISO XML documents to the ./docker/data/export folder in the Docker project. This folder will be mounted into the container, so the records can be loaded into the pycsw database.\nUse pycsw-admin.py to load the records into the catalogue database:\n\nContainer terminalPowerShell\n\n\npycsw-admin.py load-records -p /etc/data/export -c /etc/pycsw/pycsw.yml -y -r\n\n\ndocker exec -it pycsw bash -c `\n \"pycsw-admin.py load-records -p /etc/data/export -c /etc/pycsw/pycsw.yml -y -r\"\n\n\n\nValidate at http://localhost/collections/metadata:main/items if the records are loaded, else check logs to identify a problem.",
    "crumbs": [
      "Home",
      "Catalogue publication"
    ]
  },
  {
    "objectID": "3-catalogue-publication.html#customise-the-catalogue-skin",
    "href": "3-catalogue-publication.html#customise-the-catalogue-skin",
    "title": "Catalogue publication",
    "section": "Customise the catalogue skin",
    "text": "Customise the catalogue skin\npycsw uses Jinja templates to build the web frontend. These are HTML documents including template language to substitute parts of the page.\nYou find 2 template files in ./docker/pycsw/. Notice in the orchestration file how the files are mounted into the container:\n\ntemplates/landing_page.html represents the home page of pycsw\ntemplates/_base.html is a main layout template which contains page header, footer and menu and wraps around all other templates\n\nOpen a template file and make some changes (colors, text, logos).\nRestart the orchestration and view the result at http://localhost.\nHave a look at the other templates available in pycsw, which can be tailored in a similar way.",
    "crumbs": [
      "Home",
      "Catalogue publication"
    ]
  },
  {
    "objectID": "3-catalogue-publication.html#summary",
    "href": "3-catalogue-publication.html#summary",
    "title": "Catalogue publication",
    "section": "Summary",
    "text": "Summary\nIn this paragraph you learned how datasets can be published into a catalogue. In the next paragraph, we’ll look at importing metadata from external sources.",
    "crumbs": [
      "Home",
      "Catalogue publication"
    ]
  },
  {
    "objectID": "5-git-cicd.html",
    "href": "5-git-cicd.html",
    "title": "Git and CI/CD",
    "section": "",
    "text": "This page introduces a number of generic IT functionalities. Which can support communities in efficient co-creation of content. Considering the previous material, a relevant scenario is: whenever a user adds or updates a dataset (metadata) in the file repository, an automated pipeline is triggered which validates the changes, converts and uploads the updated records to a catalogue.",
    "crumbs": [
      "Home",
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#introduction",
    "href": "5-git-cicd.html#introduction",
    "title": "Git and CI/CD",
    "section": "",
    "text": "This page introduces a number of generic IT functionalities. Which can support communities in efficient co-creation of content. Considering the previous material, a relevant scenario is: whenever a user adds or updates a dataset (metadata) in the file repository, an automated pipeline is triggered which validates the changes, converts and uploads the updated records to a catalogue.",
    "crumbs": [
      "Home",
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#git-content-versioning",
    "href": "5-git-cicd.html#git-content-versioning",
    "title": "Git and CI/CD",
    "section": "Git content versioning",
    "text": "Git content versioning\nIn its core Git is a code version management system, traditionally used for maintaining software codes. In case you never worked with Git before, have a look at this Git & GitHub explanation. Some users interact with Git via the command line (shell). However excellent Graphical User Interfaces exist to work with Git repositories, such as GitHub Desktop, a Git client within Visual Studio, TortoiseGit, Smartgit, and many others.\nThese days Git based coding communities like GitHub, GitLab, Bitbucket offer various services on top of Git to facilitate in co-creation of digital assets. Those services include authentication, issue management, release management, forks, pull requests and CI/CD. The types of digital assets maintained via Git vary from software, deployment scripts, configuration files, documents, website content, metadata records, up to actual datasets. Git is most effective with text based formats, which explains the popularity of formats like CSV, YAML, and Markdown.",
    "crumbs": [
      "Home",
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#cicd",
    "href": "5-git-cicd.html#cicd",
    "title": "Git and CI/CD",
    "section": "CI/CD",
    "text": "CI/CD\nContinuous Integration & Continuous delivery describe a process in which changes in software or configuration are automatically tested and delivered to a relevant environment. These processes are commonly facilitated by Git environments. With every commit to the Git repository an action is triggered which runs some tasks.",
    "crumbs": [
      "Home",
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#github-pages-exercise",
    "href": "5-git-cicd.html#github-pages-exercise",
    "title": "Git and CI/CD",
    "section": "GitHub Pages exercise",
    "text": "GitHub Pages exercise\nThis exercise introduces the CI/CD topic by setting up a basic markdown website in GitHub Pages, maintained through Git, similar as to how this workshop website is maintained. Markdown is a popular format to store text with annotations on Git. The site will be rendered by the Quarto library. Quarto is one of many platforms to generate a website from a set of markdown files. Quarto facilitates integrations with R and Python scripts for advanced content creation.\n\nCreate a new repository in your GitHub account, for example ‘My-first-CMS’.\nBefore we add any content create a branch ‘gh-pages’ on the repository, this branch will later contain the generated html sources of the website, which will be shared via https://{you}.github.io/{my-first-cms}.\nCreate a file docs/index.md and docs/about.md. Start each file with a header:\n\n---\ntitle: Hello World\nauthor: Peter pan\ndate: 2023-11-11\n---\nAdd some markdown content to each page (under the header), for example:\n# Welcome\n\nWelcome to *my website*.\n\n- I hope you enjoy it.\n- Visit also my [about](./about.md) page.\n\nNow click on Actions in the GitHub menu. Notice that GitHub has already set up a workflow to publish our content using jekyll, it should already be available at https://user.github.io/repo.",
    "crumbs": [
      "Home",
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#using-quarto",
    "href": "5-git-cicd.html#using-quarto",
    "title": "Git and CI/CD",
    "section": "Using Quarto",
    "text": "Using Quarto\nWe’ve selected an alternative to jekyll, called quarto. In order to activate Quarto you need to set a number of items yourself.\n\nCreate a file _quarto.yml into the new Git repository, with this content:\n\nproject:\n  type: website\n  render:\n    - \"*.md\"\nwebsite:\n  title: \"My first CMS\"\n  navbar:\n    left:\n      - href: index.md\n        text: Home\n      - about.md\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nFirst you need to allow the workflow-runner to make changes on the repository. For this, open Settings, Actions, General. Scroll down to Workflow permissions. Tick the Read and write permissions and click Save. If the option is grayed out, you first need to allow this feature in your organization.\nThen, from Actions, select New workflow, then set up a workflow yourself.\nOn the next page we will create a new workflow script, which is stored in the repository at /.github/workflows/main.yml.\n\nname: Docs Deploy\n\non:\n  push:\n    branches: \n      - main\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with: \n          tinytex: true \n          path: docs\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n          path: docs\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      \n\nSave the file, via actions you can follow the progress of the workflow at every push to the repository.\nOn the logs notice how a container is initialised, the source code is checked out, the quarto dependency is installed, the build is made and pushed to the gh-pages branch.\n\nNotice that the syntax to define workflows is different for every CI/CD platform, however they generally follow a similar pattern. For GitHub identify in the file above:\n\nIt defines at what events the workflow should trigger (in this case at push events).\na build job is triggered, which indicates a container image (runs-on) to run the job in, then triggers some steps.\nThe final step triggers a facility of quarto to publish its output to a GitHub repository\n\nThe above setup is optimal for co-creating a documentation repository for your community. Users can visit the source code via the edit on GitHub link and suggest improvements via issues of pull requests. Notice that this tutorial is also maintained as markdown in Git.",
    "crumbs": [
      "Home",
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#update-catalogue-from-git-cicd",
    "href": "5-git-cicd.html#update-catalogue-from-git-cicd",
    "title": "Git and CI/CD",
    "section": "Update catalogue from Git CI/CD",
    "text": "Update catalogue from Git CI/CD\nFor this scenario we need a database in the cloud to host our records (which is reachable by GitHub actions). For the training consider to set up a FREE account at supabase.com.\n\nAt supabase, create a new account.\nThen create a new Instance of type Tiny (free).\nClick on the instance and notice the relevant connection string (URL) and password\nConnect your instance of pycsw to this database instance, by updating pycsw.cfg and following the instructions at Catalogue publication\nVerify in supabase dashboard if the records are correctly loaded.\n\nWe will now publish our records from GitHub to our database.\n\nCreate a new repository on GitHub for the records\nMake sure git-scm (or a GUI tool like Git kraken or Smartgit) is intalled on your system.\nClone (download) the repository to a local folder.\n\ngit clone https://github.com/username/records-repo.git\n\nCopy the MCF files, which have been generated in Catalogue publication, to a data folder in the cloned docker repository.\nCommit & push the files\n\ngit add -A && git commit -m \"Your Message\"\nBefore you can push your changes to GitHub, you need to set up authentication, generally 2 options are possible: - Using a personal access token - Or using SSH public key\ngit push origin main\nWe’ll now set up CI/CD to publish the records\n\nPlace the pycsw.cfg file in the root of the repository (including the postgres database connection)\nCreate a new custom workflow file with this content:\n\nname: Records Deploy\n\non: \n  push:\n    paths:\n      - '**'\n\ndefaults:\n  run:\n    working-directory: .\n\njobs:\n  build:\n    name: Build and Deploy Records\n    runs-on: ubuntu-latest\n    steps:\n        - uses: actions/checkout@v3\n        - uses: actions/setup-python@v4\n          with:\n              python-version: 3.9\n        - name: Install dependencies\n          run: |\n            sudo add-apt-repository ppa:ubuntugis/ppa\n            sudo apt-get update\n            sudo apt-get install gdal-bin\n            sudo apt-get install libgdal-dev\n            ogrinfo --version\n            pip3 install GDAL==3.4.3\n            pip3 install geodatacrawler pycsw sqlalchemy\n        - name: Crawl metadata\n          run: |\n            export pgdc_webdav_url=http://localhost/collections/metadata:main/items\n            export pgdc_canonical_url=https://github.com/pvgenuchten/data-training/tree/main/datasets/\n            crawl-metadata --dir=./datasets --mode=export --dir-out=/tmp\n        - name: Publish records\n          run: |   \n            pycsw-admin.py delete-records --config=./pycsw.yml -y\n            pycsw-admin.py load-records --config=./pycsw.yml  --path=/tmp\n\nVerify that the records are loaded on pycsw (through postgres)\nChange or add some records to Git, and verify if the changes are published (may take some time)\n\nNormally, we would not add a connection string to a database in a config file posted on GitHub. Instead GitHub offers secrets to capture this type of information.",
    "crumbs": [
      "Home",
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#cross-linking-catalogue-and-git",
    "href": "5-git-cicd.html#cross-linking-catalogue-and-git",
    "title": "Git and CI/CD",
    "section": "Cross linking catalogue and Git",
    "text": "Cross linking catalogue and Git\nWhile users are browsing the catalogue (or this page), they may find irregularities in the content. They can flag this as an issue in the relevant Git repository. A nice feature is to add a link in the catalogue page which brings them back to the relevant MCF in the Git repository. With proper authorizations they can instantly improve the record, or suggest an improvement via an issue or pull request.",
    "crumbs": [
      "Home",
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#summary",
    "href": "5-git-cicd.html#summary",
    "title": "Git and CI/CD",
    "section": "Summary",
    "text": "Summary\nIn this section you learned about using actions in GitHub (CI/CD). In the next section we are diving into data publication. Notice that you can also use Git CI/CD mechanisms to deploy or evaluate metadata and data services.",
    "crumbs": [
      "Home",
      "Git actions"
    ]
  },
  {
    "objectID": "7-providing-map-services.html",
    "href": "7-providing-map-services.html",
    "title": "Providing convenience APIs",
    "section": "",
    "text": "For spatial datasets it is of interest to also share them via convenience APIs, so the datasets can be downloaded in parts or easily be visualized in common tools such as QGIS, OpenLayers or Leaflet. The standards on data services of the Open Geospatial Consortium are designed with this purpose. These APIs give direct access to subsets or map visualizations of a dataset.\nIn this paragraph you will be introduced to various standardised APIs, after which we introduce an approach to publish datasets, which builds on the data management approach introduced in the previous paragraphs.\nThese days novel ways to share data over the web arrive, where the data formats itself allow requesting subsets of the data, enabling efficient consumption of the data straight from a repository or cloud storage service. Typical examples are Cloud Optimized GeoTiff (COG) and GeoZarr for grid data and for vector data there is GeoParquet.",
    "crumbs": [
      "Home",
      "Map services"
    ]
  },
  {
    "objectID": "7-providing-map-services.html#introduction",
    "href": "7-providing-map-services.html#introduction",
    "title": "Providing convenience APIs",
    "section": "",
    "text": "For spatial datasets it is of interest to also share them via convenience APIs, so the datasets can be downloaded in parts or easily be visualized in common tools such as QGIS, OpenLayers or Leaflet. The standards on data services of the Open Geospatial Consortium are designed with this purpose. These APIs give direct access to subsets or map visualizations of a dataset.\nIn this paragraph you will be introduced to various standardised APIs, after which we introduce an approach to publish datasets, which builds on the data management approach introduced in the previous paragraphs.\nThese days novel ways to share data over the web arrive, where the data formats itself allow requesting subsets of the data, enabling efficient consumption of the data straight from a repository or cloud storage service. Typical examples are Cloud Optimized GeoTiff (COG) and GeoZarr for grid data and for vector data there is GeoParquet.",
    "crumbs": [
      "Home",
      "Map services"
    ]
  },
  {
    "objectID": "7-providing-map-services.html#standardised-data-apis",
    "href": "7-providing-map-services.html#standardised-data-apis",
    "title": "Providing convenience APIs",
    "section": "Standardised data APIs",
    "text": "Standardised data APIs\nStandardised mapping APIs, such as Web Map Service (WMS), Web Feature service (WFS) and Web Coverage Service (WCS), originate from the beginning of this century. In recent years several challenges have been identified around these standards, which led to a series of Spatial Data on the Web Best Practices. Combined with the OGC Open Geospatial APIs - White Paper, OGC then initiated a new generation of standards based on these best practices.\nAn overview of both generations:\n\n\n\nOWS\nOGC-API\nDescription\n\n\n\n\nWeb Map Service (WMS)\nMaps\nProvides a visualization of a subset of the data\n\n\nWeb Feature Service (WFS)\nFeatures\nAPI to request a subset of the vector features\n\n\nWeb Coverage Service (WCS)\nCoverages\nAPI to interact with grid sources\n\n\nSensor Observation Service (SOS)\nSensorThings\nRetrieve subsets of sensor observations\n\n\nWeb Processing Service (WPS)\nProcesses\nRun processes on data ]\n\n\nCatalogue Service for the Web (CSW)\nRecords\nRetrieve and filter catalogue records\n\n\n\nNotice that most of the mapping software supports the standards of both generations. However, due to their recent introduction, expect incidental challenges in the implementations of OGC APIs.",
    "crumbs": [
      "Home",
      "Map services"
    ]
  },
  {
    "objectID": "7-providing-map-services.html#setting-up-an-api",
    "href": "7-providing-map-services.html#setting-up-an-api",
    "title": "Providing convenience APIs",
    "section": "Setting up an API",
    "text": "Setting up an API\nMapServer is server software which is able to expose datasets through various APIs. Examples of similar software are QGIS server, ArcGIS Server, GeoServer and pygeoapi.\nWe’ve selected MapServer for this training, because of its robustness, ease of configuration and low resource consumption. MapServer is configured using a configuration file: called the mapfile. The mapfile defines metadata for the dataset and how users interact with the dataset, mainly the colour scheme (legend) to draw a map of a dataset.\nVarious tools exist to write these configuration files, such as MapServer studio, QGIS Bridge, up to a Visual Studio plugin to edit mapfiles.\nThe pyGeoDataCrawler, introduced in a previous paragraph, also has an option to generate mapfiles. A big advantage of this approach is the integration with existing metadata. pyGeoDataCrawler will, during mapfile generation, use the existing metadata, but also update the metadata so it includes a link to the MapServer service endpoint. This toolset enables a typical workflow of:\n\nUsers find a dataset in a catalogue\nThen open the dataset via the linked service\n\nBut also vice versa; from a mapping application, access the metadata describing a dataset.",
    "crumbs": [
      "Home",
      "Map services"
    ]
  },
  {
    "objectID": "7-providing-map-services.html#mapfile-creation-exercise",
    "href": "7-providing-map-services.html#mapfile-creation-exercise",
    "title": "Providing convenience APIs",
    "section": "Mapfile creation exercise",
    "text": "Mapfile creation exercise\n\nNavigate with shell to a folder with data files.\nVerify if MCFs are available for the files, if not, create initial metadata with crawl-metadata --mode=init --dir=.\nAdd a index.yml file to the folder. This metadata is introduced in the mapfile to identify the service.\n\nmcf:\n   version: 1.0\nidentification:\n    title: My new map service\n    abstract: A map service for data about ...\ncontact:\n  pointOfContact:\n    organization: example\n    email: info@example.com\n    url: https://www.example.com\n\nSet some environment variables in the .env file; pgdc_md_url, pgdc_ms_url. Notice in the commands below that we include the .env file in the container.\nGenerate the mapfile\n\n\nLocalDocker & LinuxDocker & PowerShell\n\n\ncd /srv/data/foss4g\ncrawl-maps --dir=.\n\n\ncd ./docker/\ndocker run -it --rm --env-file=.env -v $(pwd):/tmp \\\n  pvgenuchten/geodatacrawler crawl-maps --dir=/tmp/data/foss4g \n\n\ndocker run -it --rm --env-file=.env -v \"${PWD}:/tmp\" `\n  pvgenuchten/geodatacrawler crawl-maps --dir=/tmp/data/foss4g \n\n\n\nTest your MapServer configuration. The MapServer container includes a test tool for this purpose. With the Docker composition running, try:\n\nLocalDocker & LinuxDocker & PowerShell\n\n\nmap2img \n\n\ndocker exec mapserver map2img -m /srv/data/data/data.map \\\n  -l cities -o /srv/data/data/test.png\n\n\ndocker exec mapserver map2img -m /srv/data/data/data.map `\n  -l cities -o /srv/data/data/test.png\n\n\n\nReplace -l (layer) for a layer in your mapfile. Notice a file test.png being written to the data folder.\nThe pyGeoDataCrawler tool internally uses the mappyfile library. Mappyfile is a library to work with mapfiles from Python or the comandline. It offers mapfile creation, formatting and validation options. As an example, use below code to ‘validate’ a map file.\npip3 install mappyfile\nmappyfile validate ./data/text.map --no-expand",
    "crumbs": [
      "Home",
      "Map services"
    ]
  },
  {
    "objectID": "7-providing-map-services.html#mapserver-via-docker",
    "href": "7-providing-map-services.html#mapserver-via-docker",
    "title": "Providing convenience APIs",
    "section": "MapServer via Docker",
    "text": "MapServer via Docker\nFor this workshop we’re using a MapServer image provided by Camp to Camp available from Docker Hub.\ndocker pull camptocamp/mapserver:8.4  \nFirst update the config file ./data/ms.conf. On this config file list all the mapfiles wihich are published on the container. Open the file ./data/ms.conf and populate the maps section. The maps section are key-value pairs of alias and path to the mapfile, the alias is used as http://localhost/ows/{alias}/ogcapi (for longtime MapServer users, the alias replaces the ?map=example.map syntax).\nNotice that our local ./docker/data folder is mounted into the MapServer container as /srv/data. You may have to move some content from previous paragraphs into ./docker/data folder. An alias for the foss4 map has already been added.\nMAPS\n     \"foss4g\" \"/srv/data/foss4g.map\"\nEND\nRun or restart Docker Compose.\nCheck http://localhost/ows/foss4g/ogcapi in your browser. If all has been set up fine it should show the OGCAPI homepage of the service. If not, check the container logs to evaluate any errors.\nYou can also try the url in QGIS. Add a WMS layer, of service http://localhost/ows/foss4g?request=GetCapabilities&service=WMS.\nNotice that the MCF files now include a link to the mapservice via which the datasets are shared. These links have been added by the crawl-maps method (and use the pgdc_ms_url environment variable). Publish the records to the catalogue again, so users will be able to find the service while browsing the catalogue.\npyGeoDataCrawler uses default (gray) styling for vector and an average classification for grids. You can finetune the styling of layers through the robot section in index.yml or by providing an Styled Layer Descriptor (SLD) file for a layer, as {name}.sld. Sld files can be created using QGIS (export style as SLD).",
    "crumbs": [
      "Home",
      "Map services"
    ]
  },
  {
    "objectID": "7-providing-map-services.html#summary",
    "href": "7-providing-map-services.html#summary",
    "title": "Providing convenience APIs",
    "section": "Summary",
    "text": "Summary\nIn this paragraph the standards of Open Geospatial Consortium have been introduced and how you can publish your data according to these standards using MapServer. In the next section we’ll look at measuring service quality.",
    "crumbs": [
      "Home",
      "Map services"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GitOps SDI",
    "section": "",
    "text": "This tutorial presents a workshop on setting up a Spatial Data Infrastructure (SDI) using GitOps concepts. GitOps principles aim to replace manual steps for setting up and maintaining hardware and software by introducing automated delivery pipelines from versioned Git configurations. During the workshop we introduce you to the concepts of GitOps as well as a number of OSGeo tools.\nSDIs are typically built as nodes, focused on a (spatial) data repository, a catalogue, a data access layer (APIs) and data visualization components. GitOps concepts greatly help SDI development and implementation in the following ways:\n\nContainerization\nReproducible and portable setup and deployment\nLow barrier access to modern tools for GIS, Earth observation, climate, Earth system prediction, and other data sciences\n\nThe SDI introduced in this tutorial is configured from scripts stored in Git and is delivered using a set of Docker containers running locally, on a virtual machine (VM) or in a cloud infrastructure.\nThis tutorial introduces the following OSGeo tools:\n\nPyGeoDataCrawler (pygeometa/OWSLib/Mappyfile/GDAL)\npycsw\nMapServer\nTerriaJS\n\nGit and additional services provided by the common Git service providers (GitHub, GitLab, Bitbucket, …) have a central role in the training. They are, for example, suggested to facilitate software development and delivery, content co-creation and management, as well as community feedback.\nExperience with Git and Docker are required before starting this workshop.\nIn the first paragraph we will introduce you to our approach to integrated metadata management, the core of any SDI. You can also access the slides of the workshop.\nWe hope you enjoy the materials,\nPaul and Tom.",
    "crumbs": [
      "Home"
    ]
  }
]