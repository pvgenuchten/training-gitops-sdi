[
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Slides training on spatial data flows",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nJun 15, 2025\n\n\n1.1 Training Overview\n\n\nPaul van Genuchten\n\n\n\n\n\n\nJun 15, 2025\n\n\n1.3 Tools\n\n\nPaul van Genuchten\n\n\n\n\n\n\nJun 15, 2025\n\n\n2.1 Collect and Publish metadata\n\n\nPaul van Genuchten\n\n\n\n\n\n\nJun 15, 2025\n\n\n3.1 Data services\n\n\nPaul van Genuchten\n\n\n\n\n\n\nJun 15, 2025\n\n\n3.2 Service Quality\n\n\nPaul van Genuchten\n\n\n\n\n\n\nJun 15, 2025\n\n\n4.1 Soil Profile Datamanagement\n\n\nPaul van Genuchten\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/3.2-service-quality.html#service-quality",
    "href": "slides/3.2-service-quality.html#service-quality",
    "title": "3.2 Service Quality",
    "section": "Service quality",
    "text": "Service quality\n\nMonitor the quality of data services is a driver for improvement\nCreate a monthly report with various statistics"
  },
  {
    "objectID": "slides/3.2-service-quality.html#usage",
    "href": "slides/3.2-service-quality.html#usage",
    "title": "3.2 Service Quality",
    "section": "Usage",
    "text": "Usage\n\nAggregate and display access logs\nHow are people using the services\nWhich aspects are popular\nWhich components have a bad performance"
  },
  {
    "objectID": "slides/3.2-service-quality.html#availability",
    "href": "slides/3.2-service-quality.html#availability",
    "title": "3.2 Service Quality",
    "section": "Availability",
    "text": "Availability\n\nHow often are our services available\nReceive a notification if a service is not available"
  },
  {
    "objectID": "slides/3.2-service-quality.html#complience",
    "href": "slides/3.2-service-quality.html#complience",
    "title": "3.2 Service Quality",
    "section": "Complience",
    "text": "Complience\n\nDo services comply with the indicated standards (wms, wfs)"
  },
  {
    "objectID": "slides/3.2-service-quality.html#security-incidents-log",
    "href": "slides/3.2-service-quality.html#security-incidents-log",
    "title": "3.2 Service Quality",
    "section": "Security incidents log",
    "text": "Security incidents log\n\nFraudulent email received\nMany requests from certain IP\nLow quality password detected\nBackup restored due to failing disk"
  },
  {
    "objectID": "slides/3.2-service-quality.html#search-engine-optimisation-sso",
    "href": "slides/3.2-service-quality.html#search-engine-optimisation-sso",
    "title": "3.2 Service Quality",
    "section": "Search engine optimisation (SSO)",
    "text": "Search engine optimisation (SSO)\n\nCan users find our materials on Search Engines?\nGoogle/Bing offers a SSO Console where you can view crawling and usage statistics"
  },
  {
    "objectID": "slides/3.2-service-quality.html#tools",
    "href": "slides/3.2-service-quality.html#tools",
    "title": "3.2 Service Quality",
    "section": "Tools",
    "text": "Tools\n\nCheck local IT to understand which tools are offered\nGeohealthcheck is a tool to check availability and complience\nAWStats is a basic tool to evaluate access logs\npingdom & uptimerobot are saas providers offering availability testing"
  },
  {
    "objectID": "slides/2.1-collect-publish-metadata.html#organisations-organise-their-data-in-file-repositories-or-databases",
    "href": "slides/2.1-collect-publish-metadata.html#organisations-organise-their-data-in-file-repositories-or-databases",
    "title": "2.1 Collect and Publish metadata",
    "section": "Organisations organise their data in file repositories or databases",
    "text": "Organisations organise their data in file repositories or databases\n\nFile hierarchy"
  },
  {
    "objectID": "slides/2.1-collect-publish-metadata.html#a-readme-in-each-folder",
    "href": "slides/2.1-collect-publish-metadata.html#a-readme-in-each-folder",
    "title": "2.1 Collect and Publish metadata",
    "section": "A readme in each folder",
    "text": "A readme in each folder\n\nAdd a readme to describe the resources in a folder\nWhy not structure the content in the readme, so also machines can read it?"
  },
  {
    "objectID": "slides/2.1-collect-publish-metadata.html#metadata-control-file",
    "href": "slides/2.1-collect-publish-metadata.html#metadata-control-file",
    "title": "2.1 Collect and Publish metadata",
    "section": "Metadata Control File",
    "text": "Metadata Control File\n\nA convention of the pygeometa community (Meteo Canada)\nYAML, easy to read by humans and machines\nA subset of the ISO19115 standard for metadata\n\n\nMCF"
  },
  {
    "objectID": "slides/2.1-collect-publish-metadata.html#pygeometa-library",
    "href": "slides/2.1-collect-publish-metadata.html#pygeometa-library",
    "title": "2.1 Collect and Publish metadata",
    "section": "pygeometa library",
    "text": "pygeometa library\n\na library to work with MCF\nexports MCF to various metadata models (iso, dcat, stac)\nhttps://github.com/geopython/pygeometa"
  },
  {
    "objectID": "slides/2.1-collect-publish-metadata.html#the-geodatacrawler-tool",
    "href": "slides/2.1-collect-publish-metadata.html#the-geodatacrawler-tool",
    "title": "2.1 Collect and Publish metadata",
    "section": "The geodatacrawler tool",
    "text": "The geodatacrawler tool\n\nDeveloped at ISRIC in the last year\nOn the shoulders of giants; GDAL, pygeometa, pandas\nGenerates MCF files for a file repository\nImports metadata from external sources\nExtracts MCF files from a repository to load it into a catalogue\nCreates map services configuration for files in the repository"
  },
  {
    "objectID": "slides/2.1-collect-publish-metadata.html#pycsw-catalogue",
    "href": "slides/2.1-collect-publish-metadata.html#pycsw-catalogue",
    "title": "2.1 Collect and Publish metadata",
    "section": "pycsw catalogue",
    "text": "pycsw catalogue\n\nA catalogue implementation in python\nLimited funtionality, easy maintenance\nWide range of supported catalogue standards, serves many communities\nDatabase backend\nSkin customisable with jinja templates"
  },
  {
    "objectID": "slides/1.1-overview.html#agenda",
    "href": "slides/1.1-overview.html#agenda",
    "title": "1.1 Training Overview",
    "section": "Agenda",
    "text": "Agenda\n\nMetadata publication\nData publication"
  },
  {
    "objectID": "9-data-visualisation.html",
    "href": "9-data-visualisation.html",
    "title": "Advanced options",
    "section": "",
    "text": "Various extensions are possible to tailor the system to your organisation needs.",
    "crumbs": [
      "Data visualisation"
    ]
  },
  {
    "objectID": "9-data-visualisation.html#terriajs",
    "href": "9-data-visualisation.html#terriajs",
    "title": "Advanced options",
    "section": "TerriaJS",
    "text": "TerriaJS\nTerriaJS is a modern web gis application, which includes a widget to query a catalogue. From the catalogue search results the data can be added to the TerriaJS map.\nThe main docker image definition can be used to build and run terriaJS locally.\ngit clone https://github.com/TerriaJS/TerriaMap\ncd TerriaMap\ndocker build -t local/terria .\ndocker run -p 3001:3001 local/terria\nVisit http://localhost:3001 to see TerriaJS in action.",
    "crumbs": [
      "Data visualisation"
    ]
  },
  {
    "objectID": "9-data-visualisation.html#summary",
    "href": "9-data-visualisation.html#summary",
    "title": "Advanced options",
    "section": "Summary",
    "text": "Summary\nWith this topic we conclude our training on data management. We hope you enjoyed the materials. Notice that the training can act as a starting point to a number of other resources. Let us know via Git issues if you have improvement suggestions for the materials.",
    "crumbs": [
      "Data visualisation"
    ]
  },
  {
    "objectID": "7-providing-mapservices.html",
    "href": "7-providing-mapservices.html",
    "title": "Providing convenience APIs",
    "section": "",
    "text": "For spatial datasets it is of interest to share them via convenience APIs, so the datasets can be downloaded in parts or easily be visualised in common tools such as QGIS, OpenLayers & Leaflet. The standards on data services of the Open Geospatial Consortium are designed with this purpose. These APIs give direct access to subsets or map visualisations of a dataset.\nIn this paragraph you will be introduced to various standardised APIs, after which we introduce an approach to publish datasets, which builds on the data management approach introduced in the previous paragraphs.",
    "crumbs": [
      "Mapservices"
    ]
  },
  {
    "objectID": "7-providing-mapservices.html#standardised-data-apis",
    "href": "7-providing-mapservices.html#standardised-data-apis",
    "title": "Providing convenience APIs",
    "section": "Standardised data APIs",
    "text": "Standardised data APIs\nStandardised mapping APIs, such as Web Map Service (WMS), Web Feature service (WFS) and Web Coverage Service (WCS), originate from the beginning of this century. In recent years several challenges have been identified around these standards, which led to a series of Spatial data on the web best practices. OGC then initiated a new generation of standards based on these best practices.\nAn overview of both generations:\n\n\n\nOWS\nOGC-API\nDescription\n\n\n\n\nWeb Map Service (WMS)\nMaps\nProvides a visualisation of a subset of the data\n\n\nWeb Feature Service (WFS)\nFeatures\nAPI to request a subset of the vector features\n\n\nWeb Coverage Service (WCS)\nCoverages\nAPI to interact with grid sources\n\n\nSensor Observation Service (SOS)\nSensorthings\nRetrieve subsets of sensor observations\n\n\nWeb Processing Service (WPS)\nOGCAPI:Processes\nRun processes on data ]\n\n\nCatalogue Service for the web (CSW)\nOGCAPI:Records\nRetrieve and filter catalogue records\n\n\n\nNotice that most of the mapping software supports the standards of both generations. However, due to their recent introduction, expect incidental challenges in the implementations of OGC APIs.",
    "crumbs": [
      "Mapservices"
    ]
  },
  {
    "objectID": "7-providing-mapservices.html#setting-up-an-api",
    "href": "7-providing-mapservices.html#setting-up-an-api",
    "title": "Providing convenience APIs",
    "section": "Setting up an API",
    "text": "Setting up an API\nMapserver is server software which is able to expose datasets through various APIs. Examples of similar software are QGIS server, ArcGIS Server, Geoserver and pygeoapi.\nWe’ve selected mapserver for this training, because of its robustness and low resource consumption. Mapserver is configured using a configuration file: called the mapfile. The mapfile defines metadata for the dataset and how users interact with the dataset, mainly the colour scheme (legend) to draw a map of the dataset.\nVarious tools exist to write these configuration files, such as Mapserver studio, GeoStyler, QGIS Bridge, up to a visual studio plugin to edit mapfiles.\nThe GeoDataCrawler, introduced in a previous paragraph, also has an option to generate mapfiles. A big advantage of this approach is the integration with existing metadata. GeoDataCrawler will, during mapfile generation, use the existing metadata, but also update the metadata so it includes a link to the mapserver service endpoint. This step enables a typical workflow of:\n\nUsers find a dataset in a catalogue\nThen open the dataset via the linked service\n\nBut also vice versa; from a mapping application, access the metadata describing a dataset.",
    "crumbs": [
      "Mapservices"
    ]
  },
  {
    "objectID": "7-providing-mapservices.html#mapfile-creation-exercise",
    "href": "7-providing-mapservices.html#mapfile-creation-exercise",
    "title": "Providing convenience APIs",
    "section": "Mapfile creation exercise",
    "text": "Mapfile creation exercise\n\nNavigate with shell to a folder with data files.\nVerify if mcf’s are available for the files, if not, create initial metadata with crawl-metadata --mode=init --dir=.\nAdd a index.yml file to the folder. This metadata is introduced in the mapfile to identify the service.\n\nmcf:\n   version: 1.0\nidentification:\n    title: My new mapservice\n    abstract: A map service for data about ...\ncontact:\n  pointOfContact:\n    organization: ISRIC\n    email: info@isric.org\n    url: https://www.isric.org\n\nSet some environment variables\n\n\nLinuxPowershell\n\n\nexport pgdc_md_url=\"https://kenya.lsc-hubs.org/collections/metadata:main/items/{0}\"\nexport pgdc_ms_url=\"http://localhost\"\nexport pgdc_webdav_url=\"https://example.com/data\"\n\n\n$pgdc_md_url=\"https://kenya.lsc-hubs.org/collections/metadata:main/items/{0}\"\n$pgdc_ms_url=\"http://localhost\"\n$pgdc_webdav_url=\"https://example.com/data\"\n\n\n\n\nGenerate the mapfile\n\n\nLocalDocker & LinuxDocker & Powershell\n\n\ncrawl-maps --dir=.\n\n\ndocker run -it --rm -v$(pwd):/tmp \\\n  org/metatraining crawl-maps --dir=/tmp \n\n\ndocker run -it --rm -v \"${PWD}:/tmp\" `\n  org/metatraining crawl-maps --dir=/tmp \n\n\n\n\nIndex.yml may include a “robot” property, to guide the crawler in how to process the folder. This section can be used to add specific crawling behaviour.\n\nmcf:\n    version: 1.0\nrobot:\n    skip-subfolders: True # indicates the crawler not to proceed in subfolders\nYou can test this mapfile locally if you have mapserver installed. On windows, consider using conda or ms4w.\nconda install -c conda-forge mapserver\nMapserver includes a map2img utility, which enables to render a map image from any mapfile.\nmap2img -m=./mymap.map -o=test.png",
    "crumbs": [
      "Mapservices"
    ]
  },
  {
    "objectID": "7-providing-mapservices.html#setup-mapserver-via-docker",
    "href": "7-providing-mapservices.html#setup-mapserver-via-docker",
    "title": "Providing convenience APIs",
    "section": "Setup mapserver via Docker",
    "text": "Setup mapserver via Docker\nFor this exercise we’re using a mapserver image available from Docker hub.\ndocker pull camptocamp/mapserver:master  \nFirst create a config file, which we’ll mount as a volume into the container. On this config file we list all the mapfiles we aim to publish on our container. Download the default config file. Open the file and unescape and populate the maps section:\nMAPS\n     \"data\" \"/srv/data/data.map\"\nEND\nAlso unescape the OGCAPI templates section\nOGCAPI_HTML_TEMPLATE_DIRECTORY \"/usr/local/share/mapserver/ogcapi/templates/html-bootstrap4/\"\nIn the next statement we mount the data folder, including the config file and indicate on which port and with which config file the container will run:\n\nLinuxPowershell\n\n\ndocker run -p 80:80 \\\n    -e MAPSERVER_CONFIG_FILE=/srv/data/mapserver.conf \\\n    -v $(pwd):/srv/data  \\\n    camptocamp/mapserver:master \n\n\ndocker run -p 80:80 `\n    -e MAPSERVER_CONFIG_FILE=/srv/data/mapserver.conf `\n    -v \"${PWD}:/srv/data\" `\n    camptocamp/mapserver:master \n\n\n\nCheck http://localhost/data/ogcapi in your browser. If all has been set up fine it should show the OGCAPI homepage of the service. If not, check the container logs to evaluate any errors.\nYou can also try the url in QGIS. Add a WMS layer, of service http://localhost/data?request=GetCapabilities&service=WMS.\nNotice the links to metadata when you open GetCapabilities in a browser.\n\n\n\n\n\n\nNote\n\n\n\nIn recent years browsers have become more strict, to prevent abuse. For that reason it is important to carefully consider common connectivity aspects, when setting up a new service. Websites running at https can only embed content from other https services, so using https is relevant. CORS and CORB can limit access to embedded resources from remote servers. Using proper CORS headers and Content type identification, is relevant to prevent CORS and CORB errors.\n\n\nGeoDataCrawler uses default (gray) styling for vector and an average classification for grids. You can finetune the styling of layers through the robot section in index.yml.",
    "crumbs": [
      "Mapservices"
    ]
  },
  {
    "objectID": "7-providing-mapservices.html#summary",
    "href": "7-providing-mapservices.html#summary",
    "title": "Providing convenience APIs",
    "section": "Summary",
    "text": "Summary\nIn this paragraph the standards of Open Geospatial Consortium have been introduced and how you can publish your data according to these standards using Mapserver. In the next section we’ll look at measuring service quality.",
    "crumbs": [
      "Mapservices"
    ]
  },
  {
    "objectID": "5-git-cicd.html",
    "href": "5-git-cicd.html",
    "title": "GIT and CI/CD",
    "section": "",
    "text": "This page introduces a number of generic Git functionalities and vendor add ons. Which can support communities in efficient co-creation of content. The page mainly focusses on the Continuous Integration & Deployment functionality, but contains many external links to introduce other aspects of Git. Considering the previous materials, a relevant ci-cd case is a set of tasks to run after a change to some of the mcf documents in a data repository, to validate the mcf’s and convert them to iso19139 and push them to a catalogue.",
    "crumbs": [
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#git-content-versioning",
    "href": "5-git-cicd.html#git-content-versioning",
    "title": "GIT and CI/CD",
    "section": "GIT content versioning",
    "text": "GIT content versioning\nIn its core GIT is a version management system traditionally used for maintaining software codes. In case you never worked with GIT before, have a look at this Git & Github explanation. Some users interact with Git via the command line (shell). However excellent Graphical User Interfaces exist to work with Git repositories, such as Github Desktop, a Git client within Visual Studio, TortoiseGit, Smartgit, and many others.\nThese days GIT based coding communities like Github, Gitlab, Bitbucket offer various services on top of Git to facilitate in co-creation of digital assets. Those services include authentication, issue management, release management, forks, pull requests and CI/CD. The types of digital assets maintained via GIT vary from software, deployment scripts, configuration files, documents, website content, metadata records up to actual datasets. Git is most effective with text based formats, which explains the popularity of formats like CSV, YAML, Markdown.",
    "crumbs": [
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#cicd",
    "href": "5-git-cicd.html#cicd",
    "title": "GIT and CI/CD",
    "section": "CI/CD",
    "text": "CI/CD\nContinuous Integration & Deployment describes a process in which changes in software or configuration are automatically tested and deployed to a relevant environment. These processes are commonly facilitated by GIT environments. With every commit to the Git repository an action is triggered which runs some tasks.",
    "crumbs": [
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#github-pages-exercise",
    "href": "5-git-cicd.html#github-pages-exercise",
    "title": "GIT and CI/CD",
    "section": "Github Pages exercise",
    "text": "Github Pages exercise\nThis exercise introduces the CI-CD topic by setting up a basic markdown website in Github Pages, maintained through Git. Markdown is a popular format to store text with annotations on Git.The site will be based on Quarto. Quarto is one of many platforms to generate a website from a markdown repository.\n\nCreate a new repository in your github account, for example ‘My first CMS’.\nBefore we add any content create a branch ‘gh-pages’ on the repository, this branch will later contain the generated html sources of the website.\nCreate file docs/index.md and docs/about.md. Start each file with a header:\n\n---\ntitle: Hello World\nauthor: Peter pan\ndate: 2023-11-11\n---\nAdd some markdown content to each page (under the header), for example:\n# Welcome\n\nWelcome to *my website*.\n\n- I hope you enjoy it.\n- Visit also my [about](./about.md) page.\n\nNow click on Actions in the github menu. Notice that Github has already set up a workflow to publish our content using jekyll, it should already be available at https://user.github.io/repo.",
    "crumbs": [
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#using-quarto",
    "href": "5-git-cicd.html#using-quarto",
    "title": "GIT and CI/CD",
    "section": "Using Quarto",
    "text": "Using Quarto\nWe’ve selected an alternative to jekyll, called quarto. In order to activate Quarto you need to set a number of items yourself.\n\nCreate a file _quarto.yml into the new git repository, with this content:\n\nproject:\n  type: website\nwebsite:\n  title: \"hello world\"\n  navbar:\n    left:\n      - href: index.md\n        text: Home\n      - about.md\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nRemove the existing workflow, generated by Github in Actions, Workflows, Remove\nFirst you need to allow the workflow-runner to make changes on the repository. For this, open Settings, Actions, General. Scroll down to Workflow permissions. Tick the Read and write permissions and click Save. If the option is grayed out, you first need to allow this feature in your organization.\nThen, from Actions, select New workflow, then set up a workflow yourself.\nOn the next page we will create a new workflow script, which is stored in the repository at /.github/workflows/main.yml.\n\nname: Docs Deploy\n\non:\n  push:\n    branches: \n      - main\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with: \n          tinytex: true \n          path: docs\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n          path: docs\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      \n\nSave the file, via actions you can follow the progress of the workflow at every push to the repository.\nOn the logs notice how a container is initialised, the source code is checked out, the quarto dependency is installed, the build is made and pushed to the gh-pages branch.\n\nNotice that the syntax to define workflows is different for every CI-CD platform, however they generally follow a similar pattern. For Github identify in the file above:\n\nIt defines at what events the workflow should trigger (in this case at push events).\na build job is triggered, which indicates a container image (runs-on) to run the job in, then triggers some steps.\nThe final step triggers a facility of quarto to publish its output to a github repository\n\nThe above setup is optimal for co-creating a documentation repository for your community. Users can visit the source code via the edit on github link and suggest improvements via issues of pull requests. Notice that this tutorial is also maintained as markdown in Git.",
    "crumbs": [
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#update-catalogue-from-git-ci-cd",
    "href": "5-git-cicd.html#update-catalogue-from-git-ci-cd",
    "title": "GIT and CI/CD",
    "section": "Update catalogue from GIT CI-CD",
    "text": "Update catalogue from GIT CI-CD\nFor this scenario we need a database in the cloud to host our records (which is reachable by github workflows). For the training we suggest to use a trial account at elephantsql.com.\n\nAt elephantsql, create a new account.\nThen create a new Instance of type Tiny (free).\nClick on the instance and notice the relevant connection string (URL) and password\nConnect your instance of pycsw to this database instance, by updating pycsw.cfg and following the instructions at Catalogue publication\nVerify in elephantsql dashboard if the records are correctly loaded.\n\nWe will now publish our records from Github to our database.\n\nCreate a new repository on Github for the records\nMake sure git-scm (or a GUI tool like Git kraken or Smartgit) is intalled on your system.\nClone (download) the repository to a local folder.\n\ngit clone https://github.com/username/records-repo.git\n\nCopy the mcf files, which have been generated in Catalogue publication, to a datasets folder in the cloned repository.\nCommit and the files\n\ngit add -A && git commit -m \"Your Message\"\nBefore you can push your changes to Github, you need to set up authentication, generally 2 options are possible: - Using a personal access token - Or using SSH public key\ngit push origin main\nWe’ll now set up CI-CD to publish the records\n\nPlace the pycsw.cfg file in the root of the repository (including the postgres database connection)\nCreate a new custom workflow file with this content:\n\nname: Records Deploy\n\non: \n  push:\n    paths:\n      - '**'\n\ndefaults:\n  run:\n    working-directory: .\n\njobs:\n  build:\n    name: Build and Deploy Records\n    runs-on: ubuntu-latest\n    steps:\n        - uses: actions/checkout@v3\n        - uses: actions/setup-python@v4\n          with:\n              python-version: 3.9\n        - name: Install dependencies\n          run: |\n            sudo add-apt-repository ppa:ubuntugis/ppa\n            sudo apt-get update\n            sudo apt-get install gdal-bin\n            sudo apt-get install libgdal-dev\n            ogrinfo --version\n            pip install GDAL==3.4.3\n            pip install geodatacrawler pycsw sqlalchemy\n        - name: Crawl metadata\n          run: |\n            export pgdc_webdav_url=http://localhost/collections/metadata:main/items\n            export pgdc_canonical_url=https://github.com/pvgenuchten/data-training/tree/main/datasets/\n            crawl-metadata --dir=./datasets --mode=export --dir-out=/tmp\n        - name: Publish records\n          run: |   \n            pycsw-admin.py delete-records --config=./pycsw.cfg -y\n            pycsw-admin.py load-records --config=./pycsw.cfg  --path=/tmp\n\nVerify that the records are loaded on pycsw (through postgres)\nChange or add some records to GIT, and verify if the changes are published (may take some time)\n\nNormally, we would not add a connection string to a database in a config file posted on Github. Instead Github offers secrets to capture this type of information.",
    "crumbs": [
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#cross-linking-catalogue-and-git",
    "href": "5-git-cicd.html#cross-linking-catalogue-and-git",
    "title": "GIT and CI/CD",
    "section": "Cross linking catalogue and GIT",
    "text": "Cross linking catalogue and GIT\nWhile users are browsing the catalogue (or this page), they may find irregularities in the content. They can flag this as an issue in the relevant Git repository. A nice feature is to add a link in the catalogue page which brings them back to the relevant mcf in the git repository. With proper authorisations they can instantly improve the record, or suggest an improvement via an issue or pull request.",
    "crumbs": [
      "Git actions"
    ]
  },
  {
    "objectID": "5-git-cicd.html#summary",
    "href": "5-git-cicd.html#summary",
    "title": "GIT and CI/CD",
    "section": "Summary",
    "text": "Summary\nIn this section you learned about using actions in Github (CI/CD). In the next section we are diving into data publication. Notice that you can also use GIT CI/CD mechanisms to deploy or evaluate metadata and data services.",
    "crumbs": [
      "Git actions"
    ]
  },
  {
    "objectID": "3-catalog-publication.html",
    "href": "3-catalog-publication.html",
    "title": "Catalogue publication",
    "section": "",
    "text": "Catalogues facilitate data discovery in 3 ways:",
    "crumbs": [
      "Catalog publication"
    ]
  },
  {
    "objectID": "3-catalog-publication.html#catalogue-frontend",
    "href": "3-catalog-publication.html#catalogue-frontend",
    "title": "Catalogue publication",
    "section": "Catalogue frontend",
    "text": "Catalogue frontend\nVarious catalogue frontends exist to facilitate dataset search, such as geonetwork, dataverse, ckan. Selecting a frontend depends on metadata format, target audience, types of data, maintenance aspects, and personal preference.\nFor this workshop we are going to use pycsw. It is a catalogue software supporting various standardised query APIs, as well as providing a basic easy-to-adjust html web interface.\nFor this exercise we assume you have docker-desktop installed on your system and running.\npycsw is available as docker image at docker hub, including an embedded SQLite database. In a production situation you will instead use a dedicated Postgres or MariaDB database for record storage.\n\nNavigate your shell to the temporary folder containing iso-xml documents. This folder will be mounted into the container, in order to load the records to the pycsw database.\n\n\nLinuxPowershell\n\n\ndocker run -p 8000:8000 \\\n   -v $(pwd):/etc/data \\\n   geopython/pycsw\n\n\ndocker run -p 8000:8000 `\n   -v \"${PWD}:/etc/data\" `\n   geopython/pycsw\n\n\n\n\nVisit http://localhost:8000\nMuch of the configuration of pycsw (title, contact details, database connection, url) is managed in a config file. Download the file to the current folder, adjust the title and restart docker with:\n\n\nLinuxPowershell\n\n\ndocker run -p 8000:8000 \\\n   -d --rm --name=pycsw \\\n   -v $(pwd):/etc/data \\\n   -v $(pwd)/pycsw.cfg:/etc/pycsw/pycsw.cfg \\\n   geopython/pycsw\n\n\ndocker run -p 8000:8000 `\n   -d --rm --name=pycsw `\n   -v \"${PWD}:/etc/data\" `\n   -v \"${PWD}/pycsw.cfg:/etc/pycsw/pycsw.cfg\" `\n   geopython/pycsw\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice -d starts the docker in the background, so we can interact with the running container. To see which instances are running (in the background) use docker ps. docker logs pycsw shows the logs of a container and docker stop pycsw stops the container. The -rm option removes the container at stop, so we can easily recreate it with additional options at next runs.\n\n\n\nFor administering the instance we use a utility called pycsw-admin.py. Notice on the calls below a reference to a relevant config file.\nFirst clear the existing database:\n\n\nContainer terminalPowershell\n\n\npycsw-admin.py delete-records -c /etc/pycsw/pycsw.cfg\n\n\ndocker exec -it pycsw bash -c \"pycsw-admin.py delete-records -c /etc/pycsw/pycsw.cfg\"\n\n\n\n\nNotice at http://localhost:8000/collections/metadata:main/items that all records are removed.\nLoad the records, which we exported as iso19139 in the previous section, to the database:\n\n\nContainer terminalPowershell\n\n\npycsw-admin.py load-records -p /etc/data/export -c /etc/pycsw/pycsw.cfg -y -r\n\n\ndocker exec -it pycsw bash -c `\n \"pycsw-admin.py load-records -p /etc/data/export -c /etc/pycsw/pycsw.cfg -y -r\"\n\n\n\n\nValidate at http://localhost:8000/collections/metadata:main/items if our records are loaded, else check logs to identify a problem.",
    "crumbs": [
      "Catalog publication"
    ]
  },
  {
    "objectID": "3-catalog-publication.html#customise-the-catalogue-skin",
    "href": "3-catalog-publication.html#customise-the-catalogue-skin",
    "title": "Catalogue publication",
    "section": "Customise the catalogue skin",
    "text": "Customise the catalogue skin\npycsw uses jinja templates to build the web frontend. These are html documents including template language to substitute parts of the page.\n\nSave the template below as a file ‘landing_page.html’ in the current directory\n\n{% extends \"_base.html\" %}\n{% block title %}{{ super() }} Home {% endblock %}\n{% block body %}\n&lt;h1&gt;Welcome to my catalogue!&lt;/h1&gt;\n&lt;p&gt;{{ config['metadata:main']['identification_abstract'] }}&lt;/p&gt;\nContinue to the records in this catalogue\n&lt;a title=\"Items\" \n    href=\"{{ config['server']['url'] }}/collections/metadata:main/items\"&gt;\n    Collections&lt;/a&gt;, or have a look at the  \n&lt;a title=\"OpenAPI\" \n      href=\"{{ config['server']['url'] }}/openapi?f=html\"&gt;Open API Document&lt;/a&gt;\n{% endblock %}\n\nWe will now replace the default template in the docker image with our template.\n\n\nLinuxPowershell\n\n\ndocker run -p 8000:8000 \\\n   -d --rm --name=pycsw \\\n   -v $(pwd):/etc/data \\\n   -v $(pwd)/pycsw.cfg:/etc/pycsw/pycsw.cfg \\\n   -v $(pwd)/landing_page.html:/etc/pycsw/ogc/api/templates/landing_page.html \\\n   geopython/pycsw\n\n\ndocker run -p 8000:8000 `\n   -d --rm --name=pycsw `\n   -v \"${PWD}:/etc/data\" `\n   -v \"${PWD}/pycsw.cfg:/etc/pycsw/pycsw.cfg\" `\n   -v \"${PWD}/landing_page.html:/home/pycsw/pycsw/pycsw/ogc/api/templates/landing_page.html\" `\n   geopython/pycsw\n\n\n\n\nView the result at http://localhost:8000\nHave a look at the other templates in pycsw\nWe published a tailored set of templates as a pycsw skin on github. This skin has been used as a starting point for the lsc-hubs catalogue skin.",
    "crumbs": [
      "Catalog publication"
    ]
  },
  {
    "objectID": "3-catalog-publication.html#summary",
    "href": "3-catalog-publication.html#summary",
    "title": "Catalogue publication",
    "section": "Summary",
    "text": "Summary\nIn this paragraph you learned how datasets can be published into a catalogue. In the next paragraph, we’ll look at importing metadata from external sources.",
    "crumbs": [
      "Catalog publication"
    ]
  },
  {
    "objectID": "1-existing-resources.html",
    "href": "1-existing-resources.html",
    "title": "Metadata at the source",
    "section": "",
    "text": "Many organisations organise their documents and datasets on a central network storage or database. These resources are usually clustered in organisational units, projects and/or years. Some files and database tables in that central storage contain embedded metadata, such as the name, size, date, author, location etc. This information supports users in understanding the context of the data source. Especially if that data at some point is migrated from its original context.\nFor those formats which do not have embedded metadata, or in order to capture additional metadata aspects. We endorse the creation of a sidecar metadata file for every resource, a dedicated metadata file sharing the name of the datasource. This approach is for example common in the ESRI community, where a .shp.xml is created alongside any .shp file, which captures some metadata elements.\nThrough the embedded metadata and sidecar concept, we endorse data scientists to document their data at the source. Since they know best how the data is produced and how it should be used.",
    "crumbs": [
      "Existing resources"
    ]
  },
  {
    "objectID": "1-existing-resources.html#standardised-metadata-models",
    "href": "1-existing-resources.html#standardised-metadata-models",
    "title": "Metadata at the source",
    "section": "Standardised metadata models",
    "text": "Standardised metadata models\nFor optimal interoperability, it is important to agree within your group on the metadata standard(s) to use in sidecar files. ESRI software for example provides an option to select the output model of the metadata. QGIS has various plugins, such as GeoCat Bridge, to work with various metadata models.\n\n\n\n\n\n\nTip\n\n\n\nDoes your organisation endorse a metadata model to describe data sources? Are your aware of tooling which can support you in creation of metadata in this model?",
    "crumbs": [
      "Existing resources"
    ]
  },
  {
    "objectID": "1-existing-resources.html#the-mcf-format",
    "href": "1-existing-resources.html#the-mcf-format",
    "title": "Metadata at the source",
    "section": "The MCF format",
    "text": "The MCF format\nWithin the geopython community a metadata format is used called the metadata control file (MCF). Aim of the format is ease of use, while providing export options to various metadata models. Many metadate models are based on XML, which makes them quite hard to read by humans. MCF is based on YAML, a text based format using indents to cluster elements. In this workshop we are using the MCF format due to its simplicity and natural fit with the use cases. A minimal sample of MCF is:\nmcf:\n    version: 1.0\n\nmetadata:\n    identifier: 9c36a048-4d28-453f-9373-94c90e101ebe\n    hierarchylevel: dataset\n    date: 2023-05-10\n\nidentification:\n    title: My favourite dataset\n    abstract: A sample dataset record to highlight the options of MCF\n    ...",
    "crumbs": [
      "Existing resources"
    ]
  },
  {
    "objectID": "1-existing-resources.html#describing-a-resource",
    "href": "1-existing-resources.html#describing-a-resource",
    "title": "Metadata at the source",
    "section": "Describing a resource",
    "text": "Describing a resource\nWhen describing a resource, consider which user groups are expected to read the information. This analyses will likely impact the style of writing in the metadata. The UK Geospatial Commission has published some practical recommendations on this topic.\nWhen tagging the dataset with keywords, preferably use keywords from controlled vocabularies like Agrovoc, Wikipedia, etc. Benefit of controlled vocabularies is that the term is not ambigue and it can be made available in multiple languages.",
    "crumbs": [
      "Existing resources"
    ]
  },
  {
    "objectID": "1-existing-resources.html#mcf-editing",
    "href": "1-existing-resources.html#mcf-editing",
    "title": "Metadata at the source",
    "section": "MCF editing",
    "text": "MCF editing\nMCF documents can best be written in a text editor like Visual Studio Code. Consider to install the YAML plugin for instant YAML validation.\nAnother option to create and update mcf files is via MDME. MDME is a web based software package providing a dynamic metadata edit form. An operational package is available at osgeo.github.io. Notice that if you install the package locally, you can customize the application to your organisational needs.\n\n\n\n\n\n\nTip\n\n\n\nImagine a dataset you have recently worked with. Then open mdme and populate the form, describing that dataset. Now save the MCF file so we can later place it in a sample data repository.\nNotice that MDME also offers capabilities to export directly as iso19139, it uses a webservice based on the tools used in this workshop.",
    "crumbs": [
      "Existing resources"
    ]
  },
  {
    "objectID": "1-existing-resources.html#summary",
    "href": "1-existing-resources.html#summary",
    "title": "Metadata at the source",
    "section": "Summary",
    "text": "Summary\nIn this section, you are introduced to a data management approach which maintains metadata at the location where the datasets are maintained, using a minimal, standards complient approach. You are introduced to the MCF metadata format. In the next section, we will go into more detail on interacting with the MCF format.",
    "crumbs": [
      "Existing resources"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html",
    "href": "2-interact-with-data-repositories.html",
    "title": "Interact with data repositories",
    "section": "",
    "text": "In this section a crawler tool is introduced which let’s you interact with the metadata in a file based data repository. For this exercise we’ve prepared a minimal data repository containing a number of Excel-, Shape- and Tiff-files. Unzip the repository to a location on disk.\nIn the root folder of the repository already exists a minimal mcf file, index.yml. This file contains some generic metadata properties which are used if a file within the repository does not include them. The tool we use is able to inherit metadata properties from this index.yml file through the file hierarchy.\nConsider to add additional index.yml files in other folders to override the values of index.yml at top level.",
    "crumbs": [
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#setup-environment",
    "href": "2-interact-with-data-repositories.html#setup-environment",
    "title": "Interact with data repositories",
    "section": "Setup environment",
    "text": "Setup environment\nThe tool we will use is based on python. It has some specific dependencies which are best installed via Conda. Conda creates a virtual python environment, so any activity will not interfere with the base python environment of your machine. Notice that the next paragraph describes an alternative approach without installing python locally.\nIf you don’t have Conda, we suggest to install mamba, or alternatively run the scripts from a python docker container. Each of the exercises below indicates an option to run the python script directly, or from a container.\nNow start a commandline or powershell with mamba or docker enabled (or add mamba to your PATH). On windows look for the Mamba prompt in start menu. First we will navigate to the folder in which we unzipped the sample data repository. Make sure you are not in the data directory but one above.\ncd {path-where-you-unzipped-zipfile}\n\nSetup in mamba\nFor mamba, we will create a virtual environment pgdc (using Python 3.10) for our project and activate it.\nmanba create --name pgdc python=3.10 \nmamba activate pgdc\nNotice that you can deactivate this environment with: mamba deactivate and you will return to the main Python environment. The tools we will install below, will not be available in the main environment.\nInstall the dependencies for the tool:\nmamba install -c conda-forge gdal\nmamba install -c conda-forge pysqlite3\nNow we will install the crawler tool, GeoDataCrawler. The tool is under active development at ISRIC and facilitates many of our data workflows. It is powered by some popular metadata and transformation libraries; OWSLib, pygeometa and GDAL.\npip install geodatacrawler\nVerify the different crawling options by typing:\ncrawl-metadata --help\n\n\nPython/GDAL via Docker\nIn case you have difficulties setting up python with gdal on your local machine (or just want to try out), an alternative approach is available, using python via Docker. Docker is a virtualisation technology which runs isolated containers within your computer.\n\nFirst install docker.\nStart the docker desktop tool.\nNow navigate to the folder where you unzipped the data repository and use the docker image to run the crawler:\n\ndocker run -it --rm pvgenuchten/geodatacrawler crawl-metadata --help\nFor advanced docker statements there are some differences between Windows commandline, Windows powershell and Linux. Use the relevant syntax for your system.",
    "crumbs": [
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#initial-mcf-files",
    "href": "2-interact-with-data-repositories.html#initial-mcf-files",
    "title": "Interact with data repositories",
    "section": "Initial MCF files",
    "text": "Initial MCF files\nThe initial task for the tool is to create for every data file in our repository a sidecar file based on embedded metadata from the resource.\n\nLocalDckr & LinuxDckr & Powershell\n\n\ncrawl-metadata --mode=init --dir=data\n\n\ndocker run -it --rm -v$(pwd):/tmp \\\n pvgenuchten/geodatacrawler crawl-metadata \\\n --mode=init --dir=tmp/data\n\n\ndocker run -it --rm -v \"${PWD}:/tmp\" `\n  pvgenuchten/geodatacrawler crawl-metadata `\n  --mode=init --dir=/tmp/data\n\n\n\nNotice that for each resource a {dataset}.yml file has been created. Open a .yml file in a text editor and review the content.",
    "crumbs": [
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#update-mcf",
    "href": "2-interact-with-data-repositories.html#update-mcf",
    "title": "Interact with data repositories",
    "section": "Update MCF",
    "text": "Update MCF\nThe update mode is meant to be run at intervals, it will update the mcf files if changes have been made on a resource.\n\nLocalDckr & LinuxDckr & Powershell\n\n\ncrawl-metadata --mode=update --dir=data\n\n\ndocker run -it --rm -v$(pwd):/tmp \\\n pvgenuchten/geodatacrawler crawl-metadata \\\n --mode=update --dir=tmp/data\n\n\ndocker run -it --rm -v \"${PWD}:/tmp\" `\n  pvgenuchten/geodatacrawler crawl-metadata `\n  --mode=update --dir=/tmp/data\n\n\n\nIn certain cases the update mode will also import metadata from remote url’s. This happens for example if the dataset-uri is a DOI. The update mode will ten fetch metadata of the DOI and push it into the MCF.",
    "crumbs": [
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#export-mcf",
    "href": "2-interact-with-data-repositories.html#export-mcf",
    "title": "Interact with data repositories",
    "section": "Export MCF",
    "text": "Export MCF\nFinally we want to export the MCF’s to actual iso19139 metadata to be loaded into a catalogue like pycsw, GeoNetwork, CKAN etc.\n\nLocalDckr & LinuxDckr & Powershell\n\n\ncrawl-metadata --mode=export --dir=data --dir-out=export --dir-out-mode=flat\n\n\ndocker run -it --rm -v$(pwd):/tmp \\\n pvgenuchten/geodatacrawler crawl-metadata \\\n --mode=export --dir=tmp/data \\\n --dir-out=export --dir-out-mode=flat\n\n\ndocker run -it --rm -v \"${PWD}:/tmp\" `\n  pvgenuchten/geodatacrawler crawl-metadata `\n  --mode=export --dir=/tmp/data `\n  --dir-out=/tmp/export --dir-out-mode=flat\n\n\n\nOpen one of the xml files and evaluate if the contact information from step 1 is available.",
    "crumbs": [
      "Data repositories"
    ]
  },
  {
    "objectID": "2-interact-with-data-repositories.html#summary",
    "href": "2-interact-with-data-repositories.html#summary",
    "title": "Interact with data repositories",
    "section": "Summary",
    "text": "Summary\nIn this paragraph you have been introduced to the geodatacrawler library. In the next section we are looking at catalogue publication.",
    "crumbs": [
      "Data repositories"
    ]
  },
  {
    "objectID": "4-bulk-import.html",
    "href": "4-bulk-import.html",
    "title": "Bulk import",
    "section": "",
    "text": "This paragraph describes approaches to import metadata from existing repositories. Including an option to import metadata from records of a spreadsheet.",
    "crumbs": [
      "Bulk import"
    ]
  },
  {
    "objectID": "4-bulk-import.html#bulk-import-from-spreadsheet",
    "href": "4-bulk-import.html#bulk-import-from-spreadsheet",
    "title": "Bulk import",
    "section": "Bulk import from spreadsheet",
    "text": "Bulk import from spreadsheet\nMany metadata initiatives tend to start from a spreadsheet. Each of the columns representa a metadata property and the rows are the individual records describing a resource. Spreadsheets have proven to be an effective medium to populate a catalogue with records initially. To facilitate this use case the GeoDataCrawler software provides an import spreadsheet method. The spreadsheet is parsed and a MCF document is generated for every row.\nSince every metadata initiative tends to have dedicated columns. A templating approach is used to convert from row to MCF. A default template is available, matching a default spreadsheet layout. If your spreadsheet layout is different, you need to adjust the template accordingly.\n\nFor this exercise we’ll use the LSC-hubs spreadsheet in combination with the LSC-hubs template. Notice that the template has the same filename, but with extension .j2. Download both files to a new folder, called csv, in your working directory.\nFrom your shell environment run this command:\n\n\nLocalDckr & LinuxDckr & Powershell\n\n\ncrawl-metadata --mode=import-csv --dir=\"./csv\"\n\n\ndocker run -it --rm -v$(pwd):/tmp \\\n  org/metatraining crawl-metadata \\\n  --mode=import-csv --dir=\"/tmp/csv\"\n\n\ndocker run -it --rm -v \"${PWD}:/tmp\" `\n  org/metatraining crawl-metadata `\n  --mode=import-csv --dir=\"/tmp/csv\"\n\n\n\n\nIf there are errors, check the paths and consider to open the CSV in Google Sheets and export it again or open it in a text editor to look for special cases. A known issue with this approach is that the crawler tool can not manage newline characters in text fields.\nOpen one of the generated MCF files to evaluate its content.\nA common spreadsheet tool is Microsoft Excel. If you open and export a spreadsheet from Excel, the CSV will use the ‘;’ character as column separator. Use the –sep=‘;’ parameter to indicate GeoDataCrawler to use this separator.\n\ncrawl-metadata --mode=import-csv --dir=\"./csv\" --sep=';'",
    "crumbs": [
      "Bulk import"
    ]
  },
  {
    "objectID": "4-bulk-import.html#bulk-import-from-an-online-location",
    "href": "4-bulk-import.html#bulk-import-from-an-online-location",
    "title": "Bulk import",
    "section": "Bulk import from an online location",
    "text": "Bulk import from an online location\nMany resources are already described elsewhere which may be of interest to add to our catalogue. For this use case some options exist to import remote metadata.\n\nBy ID\nIn many cases you want to import a selected subset of a remote catalogue. In this scenario we prepare a csv with the identifiers of the records to be imported. These identifiers are used to create a very minimal MCF file. GeoDataCrawler subsequently extends the local record with remote content. Currently supported are metadata from CSW, WMS and DOI.\n\nFor this exercise download a spreadsheet with a subset of the isric.org data repository and the relevant template to a folder isric.\nImport the CSV\n\ncrawl-metadata --mode=import-csv --dir=\"./isric\" --sep=';'\n\nThe synchronisation with remote content is implemented in GeoDataCrawler as part of the update metadata method. The update process will evaluate the dataseturi-property. If the uri refers to remote content in a supported format, the MCF will be updated against that content (remote takes preference).\n\ncrawl-metadata --mode=update --dir=\"./isric\" --resolve=true\n\nNotice the changes before and after running the script. If needed, you can remove all the MCF files and run import-csv again to restore the originals.\n\n\n\nHarvest full set\nIn case you want to harvest the full set of a remote catalogue, you can create a basic MCF in a new folder undrr and add a distribution of a CSW endpoint.\nmetadata:\n    hierarchylevel: service\n    identifier: riskprofilesundrr\ndistribution:\n  csw:\n    name: csw\n    url: http://riskprofilesundrr.org/catalogue/csw\n    type: OGC:CSW\nNow use the crawler to fetch the remote records of the catalogue.\ncrawl-metadata --mode=update --dir=\"./undrr\" --resolve=true\nYou can repeat this harvest at intervals to keep your catalogue up to date with the remote.",
    "crumbs": [
      "Bulk import"
    ]
  },
  {
    "objectID": "4-bulk-import.html#summary",
    "href": "4-bulk-import.html#summary",
    "title": "Bulk import",
    "section": "Summary",
    "text": "Summary\nWe’ve seen a number of options to import metadata from external sources. In the next section we’ll have a look at Git, a versioning system.",
    "crumbs": [
      "Bulk import"
    ]
  },
  {
    "objectID": "6-data-publication.html",
    "href": "6-data-publication.html",
    "title": "Data publication",
    "section": "",
    "text": "In order to share a dataset with colleagues, partners or the wider public. The file should be published in a shared environment. Various technologies are available to share a file on a network. To select a relevant location mainly depends on which type of users are going to access the dataset.\nThe following options exist:",
    "crumbs": [
      "Data publication"
    ]
  },
  {
    "objectID": "6-data-publication.html#persistent-identification",
    "href": "6-data-publication.html#persistent-identification",
    "title": "Data publication",
    "section": "Persistent identification",
    "text": "Persistent identification\nSelect the location carefully, prevent to regularly move a file to a new location. Because with every move, users need to be notified of the new location and documentation needs to be updated. When moving is relevant, consider to set up a forward-rule on the server which redirects users to the new location.\nAn interesting aspect of persistent identification is the choice of a domain and path name. A domain should represent enough credibility/authority (is this a trusted resource), and should be persistent for a longer period. A project name, for example, is not a good choice for a domain to publish a resource.\nA mechanism exists which facilitates file moves, without breaking their identification. Identifier providers such as DOI and ePIC enable creation of a identifier for any resource. In documentation use the provided identifier, in case the location of the resource changes, you can update the link behind the identifier. Some organisations install a identification service themselves, so they have full ownership on the domain and the contents of the service. An example of such a service is the identification service of the German Federal Government.",
    "crumbs": [
      "Data publication"
    ]
  },
  {
    "objectID": "6-data-publication.html#include-metadata",
    "href": "6-data-publication.html#include-metadata",
    "title": "Data publication",
    "section": "Include metadata",
    "text": "Include metadata\nFor optimal discoverability, it is important to combine data publication with metadata. Either via embedded metadata in the file, else with a separate metadata file. In case of a shared folder or cloud service, embed or place the metadata along the data files, so people browsing through the system can easily find it.\nThe embedded or sidecar metadata should be duplicated into catalogue software, to make it searchable by the targeted audience. This process is further described at catalogue publication.",
    "crumbs": [
      "Data publication"
    ]
  },
  {
    "objectID": "6-data-publication.html#summary",
    "href": "6-data-publication.html#summary",
    "title": "Data publication",
    "section": "Summary",
    "text": "Summary\nVarious technologies exist to share data on a network. When selecting a mechanism, evaluate if you can facilitate identifier persistence and share metadata along with the files. In the next section we’ll setup convenience APIs on data to facilitate reuse of the data.",
    "crumbs": [
      "Data publication"
    ]
  },
  {
    "objectID": "8-measure-quality.html",
    "href": "8-measure-quality.html",
    "title": "Quality of Service",
    "section": "",
    "text": "Quality of service monitoring supports data providers to understand strengths and weaknesses of a system. Aspects which are monitored are:\nQuality of service monitoring is a standard activity in IT. Therefore consult your IT department or hosting company if they have tools available to assess these aspects. Confirm with them on how to extend and/or share with you the registrations for the requested parameters. Combine these reports into monthly or quarterly reports to facilitate system maintenance and roadmap.",
    "crumbs": [
      "Measure quality"
    ]
  },
  {
    "objectID": "8-measure-quality.html#availability-monitoring",
    "href": "8-measure-quality.html#availability-monitoring",
    "title": "Quality of Service",
    "section": "Availability Monitoring",
    "text": "Availability Monitoring\nTo assess the availability of a service, it requires to monitor the availability of the service at intervals. A basic availability-test every 5 minutes is usually sufficient. Many software exists for availability monitoring, such as Zabbix, Nagios, CheckMK, pingdom.\nIn this workshop we’re introducting the a Python based monitoring solution, called GeoHealthCheck. Geohealthcheck is designed to monitor spatial data services specifically.\nThe following exercise assumes docker desktop to be installed. Alternatively you can create a personal account at https://demo.geohealthcheck.org (click register in the login page).\n\n\n\n\n\n\nTip\n\n\n\n\nStart by setting up a local GeoHealthCheck container:\n\ndocker run -p80:80 geopython/geohealthcheck\n\nVisit http://localhost\nLogin as user: admin, password: admin\nClick ADD + on the top bar right, select WMS\nAdd a WMS url, for example https://maps.isric.org/mapserv?map=/map/wrb.map\nOn the next screen add WMS Drilldown (so all layers are validated)\nClick Save and test\nWhen finished, click Details to see the test result\n\n\n\nThis test is automatically repeated at intervals (as long the service is running). You can return to the test page to view a diagram of the availability over time. You can also configure a e-mail address to recieve notifications if a service is not available.\nRead more at the GeoHealthCheck website,\nAn advanced aspect of availability is identifying broken links on your content. Many tools exist which can monitor wensites on broken links at intervals, for example W3C linkcheck. Also google search console can be used to identify broken links. Another option is to verify in your website access logs if there are any requests which returned a 404 status, if such a request has a referer url, you are able to identify which website incorrectly linked to one of your resources.\n\n\n\n\n\n\nTip\n\n\n\nVisit https://validator.w3.org/checklink and fill in a website you maintain or frequently visit. The check button starts a process to evaluate broken links.\n\n\nSometimes a machine is not able to identify if a link is broken, for example if the target does not return a typical 404 Not found message.",
    "crumbs": [
      "Measure quality"
    ]
  },
  {
    "objectID": "8-measure-quality.html#performance-capacity",
    "href": "8-measure-quality.html#performance-capacity",
    "title": "Quality of Service",
    "section": "Performance & Capacity",
    "text": "Performance & Capacity\nTo know the capacity and performance of a service you can perform some load tests prior to moving to production. An alternative approach to evaluate performance is to extract the access logs of the service into an aggregation tool like AWStats or Kibana and evaluate the number of requests exceeding the limits.\n\n\n\n\n\n\nNote\n\n\n\nA common challenge to service performance in spatial data is the provision of a WMS service on a big dataset. When requesting that dataset on a continental or national level, the server runs into problems drawing all the data at once. In such case consider to set up some cache/aggregation mechanism for larger areas. Setting proper min/max scale denominators can be a solution also.\n\n\njmeter is a utility which can run a series of performance and capacity tests on a webservice. Jmeter is a java program, which can run on most platforms.\n\nDownload the latest version from the apache website.\nUnzip the archive and run jmeter.bat from bin directory.\nFollow the build web test plan tutorial.\nCustomise the web test plan for your mapserver service\n\n\n\n\n\n\n\nNote\n\n\n\nDo not perform a load test against a production url, it wil severely impact the performance of that service.",
    "crumbs": [
      "Measure quality"
    ]
  },
  {
    "objectID": "8-measure-quality.html#usage-monitoring",
    "href": "8-measure-quality.html#usage-monitoring",
    "title": "Quality of Service",
    "section": "Usage monitoring",
    "text": "Usage monitoring\nTo capture the usage of a service you can extract the usage logs and import them in a tool like Kibana, Splunk, Matomo or AW stats. For spatial data, it is interesting to define rules to extract the requested layer name from a WMS request.\nAWStats is a basic utility to report on service usage.\n\nNavigate to an empty folder, place a sample log file in the folder, rename the file to access.log.\nStart a container\n\n\nLinuxPowershell\n\n\ndocker run -d --restart always --publish 3000:80 \\\n --name awstats --volume $(pwd):/var/local/log:ro \\\n pabra/awstats\n\n\ndocker run -d --restart always --publish 3000:80 `\n --name awstats -v \"${PWD}:/var/local/log:ro\" `\n pabra/awstats\n\n\n\n\nParse the logs:\n\ndocker exec awstats awstats_updateall.pl now\n\nView the dashboard at http://localhost:3000, navigate to May 2015 to see the parsed logs from the sample.\n\n\n\nSearch Engine Optimisation\nAs part of usage monitoring it is also of interest to understand how users find your data via search engines. The popular search engines offer tooling to report on how the crawlers navigate through your data and how users find your services. You need to verify ownership of a domain either via a DNS property or by uploading an identification string to the website.\nYou can also use Google rich result test to extract structured data from a website. Or go to the dataset search engine to understand if and how the search engine finds your data.",
    "crumbs": [
      "Measure quality"
    ]
  },
  {
    "objectID": "8-measure-quality.html#summary",
    "href": "8-measure-quality.html#summary",
    "title": "Quality of Service",
    "section": "Summary",
    "text": "Summary\nIn this section you learned about various mechanisms to evaluate, report about and improve service quality. This concludes the main part of the workshop. In the last section we present a framework for data visualisation.",
    "crumbs": [
      "Measure quality"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Workshop on GitOps SDI",
    "section": "",
    "text": "This tutorial presents a workshop on GitObs SDI. Spatial Data Infrastructres are typically build up around a catalogue, a data access layer (api’s) and a data visualisation tool. The SDI introduced in this tutorial is fully maintained from Git and is operated using a set of docker containers running locally, on a Virtual Machine or in a Cloud infrastructure.\nOn the tutorial some common open source tools are introduced:\n\nquarto\nmapserver\npycsw\npygeometa/owslib\nTerriaJS\n\nGit has a central role in the training, it is for example suggested to facilitate software development and deployment, content co-creation and management as well as community feedback. Experience with Git and Docker are required before starting this workshop.\nYou can also access the slides of the workshop.\nWe hope you enjoy the materials,\nPaul and Tom."
  },
  {
    "objectID": "slides/1.3-tools.html#github",
    "href": "slides/1.3-tools.html#github",
    "title": "1.3 Tools",
    "section": "Github",
    "text": "Github\nService to facilitate co-creation\n\nversion history of contributions\nauthentication\nissue management\nrelease management\nContinuous integration\n\nRelated software; Git, Gitlab, Bitbucket, Codeberg"
  },
  {
    "objectID": "slides/1.3-tools.html#quarto",
    "href": "slides/1.3-tools.html#quarto",
    "title": "1.3 Tools",
    "section": "Quarto",
    "text": "Quarto\nA content management system to create websites, documentation, slides, etc.\nRelated software; Hugo, mkdocs, Jekyll, Wordpress, Drupal"
  },
  {
    "objectID": "slides/1.3-tools.html#pygeometa-geodatacrawler",
    "href": "slides/1.3-tools.html#pygeometa-geodatacrawler",
    "title": "1.3 Tools",
    "section": "pygeometa / geodatacrawler",
    "text": "pygeometa / geodatacrawler\nScripting libraries which support data management of a file repository\n\nGenerate metadata from existing data\nImport remote metadata\nIngest metadata from a file repository\nCreate mapservices (APIs) for existing data"
  },
  {
    "objectID": "slides/1.3-tools.html#pycsw",
    "href": "slides/1.3-tools.html#pycsw",
    "title": "1.3 Tools",
    "section": "pycsw",
    "text": "pycsw\nA catalogue server application providing support for multiple metadata schemes and metadata exchange standards\nRelated products; GeoNetwork, CKAN, Dataverse"
  },
  {
    "objectID": "slides/1.3-tools.html#mapserver",
    "href": "slides/1.3-tools.html#mapserver",
    "title": "1.3 Tools",
    "section": "Mapserver",
    "text": "Mapserver\nA server application providing OGC services (WMS, WFS, WCS) on various data formats\nRelated products; Geoserver, ArcGIS server, deegree, pygeoapi, QGIS server"
  },
  {
    "objectID": "slides/1.3-tools.html#terriajs",
    "href": "slides/1.3-tools.html#terriajs",
    "title": "1.3 Tools",
    "section": "TerriaJS",
    "text": "TerriaJS\nA WebGIS application including Leaflet library\ndemo\nRelated products; GeoNode, GeoMoose, MapBender, Oskari, GisQuick"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#what-is-ogc",
    "href": "slides/3.1-ogc-data-services.html#what-is-ogc",
    "title": "3.1 Data services",
    "section": "What is OGC?",
    "text": "What is OGC?\n\nOpen Geospatial Consortium defines standards for the geospatial industry\nMembers are software companies, universities and government\nSuch as WMS, GML, OGCAPI, EPSG, GeoSPARQL\nRead more at https://opengeospatial.org"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#ows-vs-ogc-api",
    "href": "slides/3.1-ogc-data-services.html#ows-vs-ogc-api",
    "title": "3.1 Data services",
    "section": "OWS vs OGC API",
    "text": "OWS vs OGC API\n\nThe GML and OWS standards are defined on 20**\nSince then the internet moved away from xml\nin 2017 started spatial data on the web experiment with W3C, leading to OGC API\nCurrently under development\nBasic support in new and existing products\nThe browser as a tool to browse spatial data"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#ows-vs-ogc-api-1",
    "href": "slides/3.1-ogc-data-services.html#ows-vs-ogc-api-1",
    "title": "3.1 Data services",
    "section": "OWS vs OGC API",
    "text": "OWS vs OGC API\n\nGML - GeoJSON (json-fg) & HTML\nLong querystrings - Rest api (path parameters)\nGetCapabilities - Open API Specification\nFormat=xxx - Content negotiation (accept:text/html)\nFull featured - Minimal standard with extensions"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#wfs-and-ogc-api---features",
    "href": "slides/3.1-ogc-data-services.html#wfs-and-ogc-api---features",
    "title": "3.1 Data services",
    "section": "WFS and OGC API - Features",
    "text": "WFS and OGC API - Features\n\nProvides access to vector features.\nFilter, sorting and pagination options\n\n\nwms"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#wms-and-ogc-api---maps",
    "href": "slides/3.1-ogc-data-services.html#wms-and-ogc-api---maps",
    "title": "3.1 Data services",
    "section": "WMS and OGC API - Maps",
    "text": "WMS and OGC API - Maps\n\nImage rendering of subsets of the data using some styling rules\n\n\nwms"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#wmts-tms-and-ogc-api---tiles",
    "href": "slides/3.1-ogc-data-services.html#wmts-tms-and-ogc-api---tiles",
    "title": "3.1 Data services",
    "section": "WMTS, TMS and OGC API - Tiles",
    "text": "WMTS, TMS and OGC API - Tiles\n\nImage or vector representation in tiles on a grid\n\n\nwms"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#wcs-and-ogc-api---coverages",
    "href": "slides/3.1-ogc-data-services.html#wcs-and-ogc-api---coverages",
    "title": "3.1 Data services",
    "section": "WCS and OGC API - Coverages",
    "text": "WCS and OGC API - Coverages\n\nGrid extracts and calculations\n\n\nwms"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#csw-ogc-api---records-and-stac",
    "href": "slides/3.1-ogc-data-services.html#csw-ogc-api---records-and-stac",
    "title": "3.1 Data services",
    "section": "CSW, OGC API - Records and STAC",
    "text": "CSW, OGC API - Records and STAC\n\nCatalogue records\nSTAC and OGC API - Records both based on OGC API - Features"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#mapserver",
    "href": "slides/3.1-ogc-data-services.html#mapserver",
    "title": "3.1 Data services",
    "section": "Mapserver",
    "text": "Mapserver\n\nRecent versions of Mapserver support the old and new standards\nFeatures and Maps\nNo tiles (mapproxy), No coverages yet\nNo records/stac, by design"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#mapfile",
    "href": "slides/3.1-ogc-data-services.html#mapfile",
    "title": "3.1 Data services",
    "section": "Mapfile",
    "text": "Mapfile\n\nConfiguration via a mapfile\nAll gdal formats supported\nCreate mapfiles using QGIS with geocat bridge\n\n\nwms"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#sensorthings-api",
    "href": "slides/3.1-ogc-data-services.html#sensorthings-api",
    "title": "3.1 Data services",
    "section": "Sensorthings API",
    "text": "Sensorthings API\n\nSoil profile data is observation oriented (iso28258)\nSensorthings API (STA) is an OGC specification for sensor data exchange\nFrost server is an implementation of STA\nLimited availability of clients for STA, Qgis has a STA plugin"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#cloud-optimised",
    "href": "slides/3.1-ogc-data-services.html#cloud-optimised",
    "title": "3.1 Data services",
    "section": "Cloud optimised",
    "text": "Cloud optimised\n\nRecent interest in cloud optimised and cloud native formats\nLarge data files are places in a cloud bucket, users request subsets of the file using range requests\nFor grids, Cloud Optimised GeoTiff (COG) and GeoZarr (cloud native)\nFor vector, GeoParquet"
  },
  {
    "objectID": "slides/3.1-ogc-data-services.html#sematic-web",
    "href": "slides/3.1-ogc-data-services.html#sematic-web",
    "title": "3.1 Data services",
    "section": "Sematic web",
    "text": "Sematic web\n\nTo integrate spatial data in the web of knowledge, semantic web standards are relevant\nDCAT for metadata\nGeoSPARQL to define geometries\nSOSA for sensor data\nGLOSIS Web Ontology is a soil ontology based on SOSA and GeoSPARQL"
  },
  {
    "objectID": "slides/4.1-soil-profile-data-management.html#history",
    "href": "slides/4.1-soil-profile-data-management.html#history",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "History",
    "text": "History\nVarious efforts have been made to harmonise Soil Profile data\n\nGlobalSoilMap\ne-SOTER\nISO28258\nGLOSIS Web ontology"
  },
  {
    "objectID": "slides/4.1-soil-profile-data-management.html#e-soter",
    "href": "slides/4.1-soil-profile-data-management.html#e-soter",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "e-SOTER",
    "text": "e-SOTER\n\nRelational database model, usually in Microsoft Access\nKenya\nSouthern Africa\nSenegal and Gambia"
  },
  {
    "objectID": "slides/4.1-soil-profile-data-management.html#ken-soter",
    "href": "slides/4.1-soil-profile-data-management.html#ken-soter",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "KEN-SOTER",
    "text": "KEN-SOTER\n\n\n\n\n\nKEN SOTER map\n\n\n\n\n\n\nKEN SOTER model"
  },
  {
    "objectID": "slides/4.1-soil-profile-data-management.html#iso28258",
    "href": "slides/4.1-soil-profile-data-management.html#iso28258",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "ISO28258",
    "text": "ISO28258\n\n\n\nISO28258 is based on Observations and measurements\nevery field observation and laboratory analysis as a sensor observation\n\n\n\n\n\nO&M model"
  },
  {
    "objectID": "slides/4.1-soil-profile-data-management.html#iso28258-in-gml",
    "href": "slides/4.1-soil-profile-data-management.html#iso28258-in-gml",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "ISO28258 in GML",
    "text": "ISO28258 in GML\n\nSerialisation of ISO28258 in GML (app schema)\nQuite complex implementation and low support in tooling\nINSPIRE Soil (Europe)\n\nANZSoilML (Australia)\nAlternative is a serialisation in JSON using Sensorthings API."
  },
  {
    "objectID": "slides/4.1-soil-profile-data-management.html#iso28258-as-relational-database",
    "href": "slides/4.1-soil-profile-data-management.html#iso28258-as-relational-database",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "ISO28258 as relational database",
    "text": "ISO28258 as relational database\n\nA relational model is available according to ISO28258\nEither as PostGreSQL or GeoPackage (SQLite)\nRead more at https://iso28258.isric.org"
  },
  {
    "objectID": "slides/4.1-soil-profile-data-management.html#glosis-web-ontology",
    "href": "slides/4.1-soil-profile-data-management.html#glosis-web-ontology",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "GLOSIS Web Ontology",
    "text": "GLOSIS Web Ontology\n\nGLOSIS Web Ontology has expressed ISO28258 using common web ontologys (SOSA, VCARD)\nIncludes codelists for soil properties and procedures"
  },
  {
    "objectID": "slides/4.1-soil-profile-data-management.html#harmonisation-of-field-lab-and-legacy-data",
    "href": "slides/4.1-soil-profile-data-management.html#harmonisation-of-field-lab-and-legacy-data",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "Harmonisation of field, lab and legacy data",
    "text": "Harmonisation of field, lab and legacy data\n\nUse common codelists to identify properties, methods, units\nHale Studio and FME are tools for data harmonisation\nAlso SQL scripts are often used to harmonise data\nSemantic web community offers harmonisation tools (rml, tarql, csv-ld)"
  }
]